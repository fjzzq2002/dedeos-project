{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import load_data\n",
    "from load_data import GenderDataset, gender_data_collate_fn\n",
    "from models.encoder_t5 import EncoderT5\n",
    "from models.classifier_bert import ClassifierBERT\n",
    "from models.similarity_sent_enc import encode_for_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "wd = 5e-6\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, \"data\", \"blog.json\"), \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "docs = json_data['docs'][1:] # I don't want to see the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13773 2951 2952\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(docs)\n",
    "num_train_docs = int(num_docs * 0.7)\n",
    "num_val_docs = int(num_docs * 0.15)\n",
    "num_test_docs = num_docs - num_train_docs - num_val_docs\n",
    "print(num_train_docs, num_val_docs, num_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:num_train_docs]\n",
    "val_docs = docs[num_train_docs:num_train_docs+num_val_docs]\n",
    "test_docs = docs[num_train_docs+num_val_docs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13773 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 13773/13773 [02:48<00:00, 81.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 449320\n",
      "Discarded ratio (due to MIN_LENGTH): 0.165\n",
      "Number of unique words before converting to [UNK]:  27221\n",
      "Converting words with frequencies less than 10 to [UNK]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449320/449320 [00:39<00:00, 11304.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting [UNK]:  25086\n",
      "Known occurrences rate 99.98%\n",
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2951/2951 [00:33<00:00, 89.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 97159\n",
      "Discarded ratio (due to MIN_LENGTH): 0.151\n",
      "Number of unique words before converting to [UNK]:  26020\n",
      "Converting words with frequencies less than 10 to [UNK]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97159/97159 [00:07<00:00, 12872.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting [UNK]:  19062\n",
      "Known occurrences rate 99.72%\n",
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2952/2952 [00:24<00:00, 120.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 71898\n",
      "Discarded ratio (due to MIN_LENGTH): 0.164\n",
      "Number of unique words before converting to [UNK]:  25551\n",
      "Converting words with frequencies less than 10 to [UNK]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71898/71898 [00:06<00:00, 11405.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting [UNK]:  17366\n",
      "Known occurrences rate 99.57%\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = None, None, None\n",
    "load_from_pickled = False\n",
    "\n",
    "if os.path.exists(os.path.join(os.curdir, \"data\", \"train.pickle\")):\n",
    "    load_from_pickled = True\n",
    "    with open(os.path.join(os.curdir, \"data\", \"train.pickle\"), \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "    with open(os.path.join(os.curdir, \"data\", \"val.pickle\"), \"rb\") as f:\n",
    "        val_dataset = pickle.load(f)\n",
    "    with open(os.path.join(os.curdir, \"data\", \"test.pickle\"), \"rb\") as f:\n",
    "        test_dataset = pickle.load(f)\n",
    "else:\n",
    "    train_dataset = GenderDataset(train_docs)\n",
    "    val_dataset = GenderDataset(val_docs)\n",
    "    test_dataset = GenderDataset(test_docs)\n",
    "\n",
    "if not load_from_pickled:\n",
    "    with open(os.path.join(os.curdir, \"data\", \"train.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "    with open(os.path.join(os.curdir, \"data\", \"val.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(val_dataset, f)\n",
    "    with open(os.path.join(os.curdir, \"data\", \"test.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dataset, f)\n",
    "\n",
    "print(load_from_pickled)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " (tensor([[3305, 2023, 1012,  ..., 1998, 6389, 2126],\n",
       "          [2061, 3376, 1998,  ..., 1996, 2048, 2060],\n",
       "          [2041, 3782, 7632,  ..., 2000, 2131, 2070],\n",
       "          ...,\n",
       "          [2043, 2016, 2001,  ..., 2033, 1012, 5206],\n",
       "          [4532, 1045, 6293,  ..., 2061, 2919, 1010],\n",
       "          [2009, 2357, 2041,  ..., 4123, 2052, 2566]], dtype=torch.int32),\n",
       "  tensor([128, 128, 128, 128, 128, 128, 128, 128], dtype=torch.int32),\n",
       "  tensor([0, 1, 0, 0, 0, 1, 1, 1])))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = EncoderT5(\n",
    "    vocab_size=load_data.tokenizer.vocab_size\n",
    ").to(device)\n",
    "\n",
    "classifier_model = ClassifierBERT(\n",
    "    vocab_size=load_data.tokenizer.vocab_size\n",
    ")\n",
    "\n",
    "st_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "st_model[0].auto_model=st_model[0].auto_model.to(device)\n",
    "st_model[0].auto_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(list(encoder_model.parameters()) + list(classifier_model.parameters()), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_similarity = nn.MSELoss()\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_perplexity = nn.L1Loss() # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, models, criterions, optimizer, num_epoch):\n",
    "    \n",
    "    train_id = random.randint(0, 1000)\n",
    "    log = open(f'./save/{train_id}.txt', 'w')\n",
    "    models['st'].eval()\n",
    "    models['ppl'].eval()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        print(f\"Epoch {epoch}, total {len(train_dataloader)} batches\\n\")\n",
    "        log.write(f\"Epoch {epoch}, total {len(train_dataloader)} batches\\n\")\n",
    "        log.flush()\n",
    "\n",
    "        models['enc'].train()\n",
    "        models['cls'].train()\n",
    "\n",
    "        for batch, (src_ids, src_len, tgt) in enumerate(train_dataloader):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            src_ids = src_ids.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            # Calculate the obfuscated logits via the encoder\n",
    "            obf_logits = models['enc'](src_ids)\n",
    "            \n",
    "            src_encode, obf_encode = encode_for_similarities(models['st'], src_ids, obf_logits)\n",
    "            loss_sim = criterions['sim'](src_encode, obf_encode)\n",
    "\n",
    "            gender_logits = models['cls'](src_ids)\n",
    "            loss_cls = criterions['cls'](gender_logits, tgt)\n",
    "\n",
    "            loss_ppl = criterions['ppl'](gender_logits, tgt) # Placeholder\n",
    "\n",
    "            loss = loss_sim + loss_cls + loss_ppl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"\\nBegin Evaluation\")\n",
    "        models['enc'].eval()\n",
    "        models['cls'].eval()\n",
    "        total_acc = 0\n",
    "        total_ppl = 0\n",
    "        total_sim = 0\n",
    "        limit=len(val_dataloader)\n",
    "        #if limit>100 and batch: limit=100\n",
    "        with torch.no_grad():\n",
    "            for batch, (src_ids, src_len, tgt) in tqdm(enumerate(val_dataloader),total=limit):\n",
    "                src_ids = src_ids.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "\n",
    "                obf_logits = models['enc'](src_ids)\n",
    "                src_encode, obf_encode = encode_for_similarities(models['st'], src_ids, obf_logits)\n",
    "                gender_logits = models['cls'](src_ids)\n",
    "\n",
    "                total_acc += (gender_logits.argmax(1) == tgt).sum.item()\n",
    "                total_ppl += 1 # placeholder\n",
    "                total_sim += torch.sum(torch.mean(src_encode * obf_encode, dim=1))\n",
    "\n",
    "                if batch >= limit:\n",
    "                    break\n",
    "        \n",
    "        acc = total_acc / limit / batch_size\n",
    "        ppl = total_ppl / limit / batch_size\n",
    "        sim = total_sim / limit / batch_size\n",
    "\n",
    "        print(f\"Validation Accuracy: {acc}\\nValidation Perplexity: {ppl}\\nValidation Semantic Similarity: {sim}\\n\")\n",
    "        log.write(f\"Validation Accuracy: {acc}\\nValidation Perplexity: {ppl}\\nValidation Semantic Similarity: {sim}\\n\")\n",
    "        log.flush()\n",
    "\n",
    "        torch.save(models['cls'].state_dict(), f'./save/classifier_model_{train_id}_{batch_size}_epoch_{epoch}.file')\n",
    "        torch.save(models['enc'].state_dict(), f'./save/encoder_model_{train_id}_{batch_size}_epoch_{epoch}.file')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ecdeeea834996ab17c476b9c82385c086ccb21d059ebd3afa1913627f92baf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
