{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from load_data import Tokenizer, GenderDataset, gender_data_collate_fn\n",
    "from models.classifier_lstm import ClassifierLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_embedding_size = 512\n",
    "classifier_hidden_size = 512\n",
    "classifier_num_layers = 2\n",
    "classifier_is_bidirectional = True\n",
    "classifier_dropout = 0.6\n",
    "\n",
    "classifier_lr = 3e-4\n",
    "classifier_weight_decay = 1e-3\n",
    "classifier_num_epoch = 100\n",
    "classifier_batch_size = 128\n",
    "classifier_max_norm = 2\n",
    "\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, \"data\", \"blog.json\"), \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "docs = json_data['docs'][1:] # I don't want to see the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19676/19676 [00:52<00:00, 377.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 559126\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 559126/559126 [00:26<00:00, 21398.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 554016\n",
      "Number of unique words before converting to <UNK>:  505954\n",
      "Converting words with frequencies less than 20 to <UNK>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "554016it [00:17, 30898.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting <UNK>:  40741\n",
      "Known occurrences rate 98.26%\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13773 2951 2952\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(docs)\n",
    "num_train_docs = int(num_docs * 0.7)\n",
    "num_val_docs = int(num_docs * 0.15)\n",
    "num_test_docs = num_docs - num_train_docs - num_val_docs\n",
    "print(num_train_docs, num_val_docs, num_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:num_train_docs]\n",
    "val_docs = docs[num_train_docs:num_train_docs+num_val_docs]\n",
    "test_docs = docs[num_train_docs+num_val_docs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2952/2952 [00:07<00:00, 405.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 65739\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65739it [00:02, 25922.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 65047\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GenderDataset(train_docs, tokenizer)\n",
    "val_dataset = GenderDataset(val_docs, tokenizer)\n",
    "test_dataset = GenderDataset(test_docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "classifier_model = ClassifierLSTM(\n",
    "    tokenizer.vocab_size(), \n",
    "    classifier_embedding_size, \n",
    "    classifier_hidden_size, \n",
    "    classifier_num_layers, \n",
    "    classifier_is_bidirectional\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(classifier_model.parameters(), lr = classifier_lr, weight_decay=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, criterion, optimizer, num_epoch):\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(f\"Epoch {epoch}, total {len(train_dataloader)} batches\\n\")\n",
    "        model.train()\n",
    "\n",
    "        for batch, (src_ids, src_len, tgt) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            src_ids = src_ids.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            logits = model(src_ids, src_len)\n",
    "            loss = criterion(logits, tgt)\n",
    "            if batch % print_every == 0:\n",
    "                print(f\"Epoch Step: {batch} Loss: {loss} Acc: {(logits.argmax(1) == tgt).sum().item() / tgt.size(0)}\")\n",
    "\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), classifier_max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\nBegin Evaluation\")\n",
    "        model.eval()\n",
    "        total_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (src_ids, src_len, tgt) in enumerate(val_dataloader):\n",
    "                src_ids = src_ids.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                logits = model(src_ids, src_len)\n",
    "                total_acc += (logits.argmax(1) == tgt).sum().item()\n",
    "        \n",
    "        acc = total_acc / len(val_dataloader.dataset)\n",
    "        print(f\"Validation Accuracy: {acc}, model saved\\n\")\n",
    "\n",
    "        torch.save(model.state_dict(), f'./save/classifier_model_clip_{classifier_hidden_size}_{classifier_batch_size}_{classifier_num_layers}_{classifier_is_bidirectional}_epoch_{epoch}.file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6946387887001038 Acc: 0.515625\n",
      "Epoch Step: 200 Loss: 0.6872751712799072 Acc: 0.578125\n",
      "Epoch Step: 400 Loss: 0.6896743178367615 Acc: 0.5390625\n",
      "Epoch Step: 600 Loss: 0.6924378871917725 Acc: 0.53125\n",
      "Epoch Step: 800 Loss: 0.7062233686447144 Acc: 0.4609375\n",
      "Epoch Step: 1000 Loss: 0.6824397444725037 Acc: 0.6015625\n",
      "Epoch Step: 1200 Loss: 0.6917186975479126 Acc: 0.5390625\n",
      "Epoch Step: 1400 Loss: 0.6932624578475952 Acc: 0.5234375\n",
      "Epoch Step: 1600 Loss: 0.7041881084442139 Acc: 0.4453125\n",
      "Epoch Step: 1800 Loss: 0.6872370839118958 Acc: 0.5546875\n",
      "Epoch Step: 2000 Loss: 0.6757984161376953 Acc: 0.6328125\n",
      "Epoch Step: 2200 Loss: 0.6879671216011047 Acc: 0.546875\n",
      "Epoch Step: 2400 Loss: 0.6847577095031738 Acc: 0.578125\n",
      "Epoch Step: 2600 Loss: 0.6857582330703735 Acc: 0.5546875\n",
      "Epoch Step: 2800 Loss: 0.6962257623672485 Acc: 0.53125\n",
      "Epoch Step: 3000 Loss: 0.6852021217346191 Acc: 0.5625\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5352959505152736, model saved\n",
      "\n",
      "Epoch 1, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6858882308006287 Acc: 0.578125\n",
      "Epoch Step: 200 Loss: 0.6879828572273254 Acc: 0.5546875\n",
      "Epoch Step: 400 Loss: 0.6988020539283752 Acc: 0.5390625\n",
      "Epoch Step: 600 Loss: 0.6880620718002319 Acc: 0.53125\n",
      "Epoch Step: 800 Loss: 0.7055015563964844 Acc: 0.453125\n",
      "Epoch Step: 1000 Loss: 0.6838172078132629 Acc: 0.5703125\n",
      "Epoch Step: 1200 Loss: 0.688737154006958 Acc: 0.5859375\n",
      "Epoch Step: 1400 Loss: 0.6837562918663025 Acc: 0.5546875\n",
      "Epoch Step: 1600 Loss: 0.699360191822052 Acc: 0.5078125\n",
      "Epoch Step: 1800 Loss: 0.6887557506561279 Acc: 0.53125\n",
      "Epoch Step: 2000 Loss: 0.6885529160499573 Acc: 0.5234375\n",
      "Epoch Step: 2200 Loss: 0.6762585639953613 Acc: 0.671875\n",
      "Epoch Step: 2400 Loss: 0.6929851770401001 Acc: 0.53125\n",
      "Epoch Step: 2600 Loss: 0.6887786984443665 Acc: 0.53125\n",
      "Epoch Step: 2800 Loss: 0.6826015114784241 Acc: 0.546875\n",
      "Epoch Step: 3000 Loss: 0.680748701095581 Acc: 0.609375\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5350536046068803, model saved\n",
      "\n",
      "Epoch 2, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6847342252731323 Acc: 0.546875\n",
      "Epoch Step: 200 Loss: 0.6929410696029663 Acc: 0.4921875\n",
      "Epoch Step: 400 Loss: 0.6787246465682983 Acc: 0.578125\n",
      "Epoch Step: 600 Loss: 0.6799684762954712 Acc: 0.5859375\n",
      "Epoch Step: 800 Loss: 0.6867793202400208 Acc: 0.5078125\n",
      "Epoch Step: 1000 Loss: 0.6864961385726929 Acc: 0.53125\n",
      "Epoch Step: 1200 Loss: 0.7023341655731201 Acc: 0.4765625\n",
      "Epoch Step: 1400 Loss: 0.6760896444320679 Acc: 0.6015625\n",
      "Epoch Step: 1600 Loss: 0.6754202842712402 Acc: 0.5703125\n",
      "Epoch Step: 1800 Loss: 0.6730366349220276 Acc: 0.5546875\n",
      "Epoch Step: 2000 Loss: 0.6821451187133789 Acc: 0.5234375\n",
      "Epoch Step: 2200 Loss: 0.6818835735321045 Acc: 0.5703125\n",
      "Epoch Step: 2400 Loss: 0.6935206055641174 Acc: 0.5\n",
      "Epoch Step: 2600 Loss: 0.6895034313201904 Acc: 0.53125\n",
      "Epoch Step: 2800 Loss: 0.6706172227859497 Acc: 0.53125\n",
      "Epoch Step: 3000 Loss: 0.6809945106506348 Acc: 0.5234375\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5184009786158587, model saved\n",
      "\n",
      "Epoch 3, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.699501633644104 Acc: 0.5234375\n",
      "Epoch Step: 200 Loss: 0.6681021451950073 Acc: 0.5703125\n",
      "Epoch Step: 400 Loss: 0.6673269271850586 Acc: 0.6484375\n",
      "Epoch Step: 600 Loss: 0.6553041934967041 Acc: 0.578125\n",
      "Epoch Step: 800 Loss: 0.6526137590408325 Acc: 0.6328125\n",
      "Epoch Step: 1000 Loss: 0.6874127984046936 Acc: 0.5546875\n",
      "Epoch Step: 1200 Loss: 0.6922256350517273 Acc: 0.53125\n",
      "Epoch Step: 1400 Loss: 0.6501462459564209 Acc: 0.609375\n",
      "Epoch Step: 1600 Loss: 0.6458472609519958 Acc: 0.625\n",
      "Epoch Step: 1800 Loss: 0.6721209287643433 Acc: 0.5625\n",
      "Epoch Step: 2000 Loss: 0.6805757284164429 Acc: 0.5390625\n",
      "Epoch Step: 2200 Loss: 0.6744260787963867 Acc: 0.609375\n",
      "Epoch Step: 2400 Loss: 0.6603682041168213 Acc: 0.578125\n",
      "Epoch Step: 2600 Loss: 0.6925328969955444 Acc: 0.4765625\n",
      "Epoch Step: 2800 Loss: 0.669915497303009 Acc: 0.5703125\n",
      "Epoch Step: 3000 Loss: 0.6814896464347839 Acc: 0.5078125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5150081358983531, model saved\n",
      "\n",
      "Epoch 4, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6071523427963257 Acc: 0.6953125\n",
      "Epoch Step: 200 Loss: 0.6110638380050659 Acc: 0.671875\n",
      "Epoch Step: 400 Loss: 0.6220937371253967 Acc: 0.6484375\n",
      "Epoch Step: 600 Loss: 0.6230919361114502 Acc: 0.6953125\n",
      "Epoch Step: 800 Loss: 0.6291681528091431 Acc: 0.6484375\n",
      "Epoch Step: 1000 Loss: 0.6179221272468567 Acc: 0.6328125\n",
      "Epoch Step: 1200 Loss: 0.6623088121414185 Acc: 0.6171875\n",
      "Epoch Step: 1400 Loss: 0.6900585293769836 Acc: 0.59375\n",
      "Epoch Step: 1600 Loss: 0.6078259348869324 Acc: 0.625\n",
      "Epoch Step: 1800 Loss: 0.6424083709716797 Acc: 0.6328125\n",
      "Epoch Step: 2000 Loss: 0.6453437805175781 Acc: 0.6328125\n",
      "Epoch Step: 2200 Loss: 0.6054409742355347 Acc: 0.671875\n",
      "Epoch Step: 2400 Loss: 0.6582255959510803 Acc: 0.6328125\n",
      "Epoch Step: 2600 Loss: 0.6261024475097656 Acc: 0.5859375\n",
      "Epoch Step: 2800 Loss: 0.6259108781814575 Acc: 0.640625\n",
      "Epoch Step: 3000 Loss: 0.6319767236709595 Acc: 0.640625\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5077492989279079, model saved\n",
      "\n",
      "Epoch 5, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6000555753707886 Acc: 0.6875\n",
      "Epoch Step: 200 Loss: 0.6119416356086731 Acc: 0.7109375\n",
      "Epoch Step: 400 Loss: 0.578711211681366 Acc: 0.6171875\n",
      "Epoch Step: 600 Loss: 0.604431688785553 Acc: 0.6953125\n",
      "Epoch Step: 800 Loss: 0.5617366433143616 Acc: 0.6875\n",
      "Epoch Step: 1000 Loss: 0.5831084847450256 Acc: 0.65625\n",
      "Epoch Step: 1200 Loss: 0.6151775121688843 Acc: 0.65625\n",
      "Epoch Step: 1400 Loss: 0.5915343165397644 Acc: 0.7109375\n",
      "Epoch Step: 1600 Loss: 0.6207561492919922 Acc: 0.625\n",
      "Epoch Step: 1800 Loss: 0.5920512080192566 Acc: 0.6796875\n",
      "Epoch Step: 2000 Loss: 0.5830055475234985 Acc: 0.6171875\n",
      "Epoch Step: 2200 Loss: 0.6009044647216797 Acc: 0.671875\n",
      "Epoch Step: 2400 Loss: 0.533417284488678 Acc: 0.6875\n",
      "Epoch Step: 2600 Loss: 0.66761314868927 Acc: 0.609375\n",
      "Epoch Step: 2800 Loss: 0.647186279296875 Acc: 0.59375\n",
      "Epoch Step: 3000 Loss: 0.5676648616790771 Acc: 0.6328125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5037679018614474, model saved\n",
      "\n",
      "Epoch 6, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.5175900459289551 Acc: 0.75\n",
      "Epoch Step: 200 Loss: 0.46348893642425537 Acc: 0.7890625\n",
      "Epoch Step: 400 Loss: 0.5143359303474426 Acc: 0.734375\n",
      "Epoch Step: 600 Loss: 0.4020584225654602 Acc: 0.8125\n",
      "Epoch Step: 800 Loss: 0.5609908103942871 Acc: 0.734375\n",
      "Epoch Step: 1000 Loss: 0.5160884261131287 Acc: 0.7421875\n",
      "Epoch Step: 1200 Loss: 0.5323415398597717 Acc: 0.703125\n",
      "Epoch Step: 1400 Loss: 0.529053270816803 Acc: 0.734375\n",
      "Epoch Step: 1600 Loss: 0.5206204652786255 Acc: 0.75\n",
      "Epoch Step: 1800 Loss: 0.5371093153953552 Acc: 0.734375\n",
      "Epoch Step: 2000 Loss: 0.474145770072937 Acc: 0.7578125\n",
      "Epoch Step: 2200 Loss: 0.4389246106147766 Acc: 0.8359375\n",
      "Epoch Step: 2400 Loss: 0.49235329031944275 Acc: 0.7421875\n",
      "Epoch Step: 2600 Loss: 0.48306772112846375 Acc: 0.75\n",
      "Epoch Step: 2800 Loss: 0.47267842292785645 Acc: 0.75\n",
      "Epoch Step: 3000 Loss: 0.5595228672027588 Acc: 0.6953125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5064106262910689, model saved\n",
      "\n",
      "Epoch 7, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.47993481159210205 Acc: 0.7890625\n",
      "Epoch Step: 200 Loss: 0.4365629553794861 Acc: 0.8125\n",
      "Epoch Step: 400 Loss: 0.42421963810920715 Acc: 0.7421875\n",
      "Epoch Step: 600 Loss: 0.4345158338546753 Acc: 0.78125\n",
      "Epoch Step: 800 Loss: 0.5427247285842896 Acc: 0.6796875\n",
      "Epoch Step: 1000 Loss: 0.43118250370025635 Acc: 0.7734375\n",
      "Epoch Step: 1200 Loss: 0.42324841022491455 Acc: 0.8125\n",
      "Epoch Step: 1400 Loss: 0.5238488912582397 Acc: 0.7734375\n",
      "Epoch Step: 1600 Loss: 0.5113152265548706 Acc: 0.7890625\n",
      "Epoch Step: 1800 Loss: 0.4768274128437042 Acc: 0.8046875\n",
      "Epoch Step: 2000 Loss: 0.5733702778816223 Acc: 0.640625\n",
      "Epoch Step: 2200 Loss: 0.42790964245796204 Acc: 0.8046875\n",
      "Epoch Step: 2400 Loss: 0.420480340719223 Acc: 0.796875\n",
      "Epoch Step: 2600 Loss: 0.4353562891483307 Acc: 0.7734375\n",
      "Epoch Step: 2800 Loss: 0.4115845859050751 Acc: 0.7890625\n",
      "Epoch Step: 3000 Loss: 0.5031383037567139 Acc: 0.7578125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5047834466204286, model saved\n",
      "\n",
      "Epoch 8, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.367362380027771 Acc: 0.859375\n",
      "Epoch Step: 200 Loss: 0.37364470958709717 Acc: 0.8046875\n",
      "Epoch Step: 400 Loss: 0.3096257150173187 Acc: 0.8671875\n",
      "Epoch Step: 600 Loss: 0.33540719747543335 Acc: 0.84375\n",
      "Epoch Step: 800 Loss: 0.3390590250492096 Acc: 0.8125\n",
      "Epoch Step: 1000 Loss: 0.49413737654685974 Acc: 0.734375\n",
      "Epoch Step: 1200 Loss: 0.3946879804134369 Acc: 0.8046875\n",
      "Epoch Step: 1400 Loss: 0.336831271648407 Acc: 0.84375\n",
      "Epoch Step: 1600 Loss: 0.36724036931991577 Acc: 0.8125\n",
      "Epoch Step: 1800 Loss: 0.3967749774456024 Acc: 0.78125\n",
      "Epoch Step: 2000 Loss: 0.3723382353782654 Acc: 0.796875\n",
      "Epoch Step: 2200 Loss: 0.342786967754364 Acc: 0.859375\n",
      "Epoch Step: 2400 Loss: 0.43068015575408936 Acc: 0.796875\n",
      "Epoch Step: 2600 Loss: 0.41319605708122253 Acc: 0.7734375\n",
      "Epoch Step: 2800 Loss: 0.34049779176712036 Acc: 0.84375\n",
      "Epoch Step: 3000 Loss: 0.5096926093101501 Acc: 0.8046875\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.506814536138391, model saved\n",
      "\n",
      "Epoch 9, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.31999558210372925 Acc: 0.8359375\n",
      "Epoch Step: 200 Loss: 0.23572686314582825 Acc: 0.890625\n",
      "Epoch Step: 400 Loss: 0.27081385254859924 Acc: 0.8828125\n",
      "Epoch Step: 600 Loss: 0.36283808946609497 Acc: 0.8515625\n",
      "Epoch Step: 800 Loss: 0.2816547155380249 Acc: 0.8515625\n",
      "Epoch Step: 1000 Loss: 0.3787813186645508 Acc: 0.8046875\n",
      "Epoch Step: 1200 Loss: 0.25497815012931824 Acc: 0.890625\n",
      "Epoch Step: 1400 Loss: 0.37783950567245483 Acc: 0.7890625\n",
      "Epoch Step: 1600 Loss: 0.31647998094558716 Acc: 0.8515625\n",
      "Epoch Step: 1800 Loss: 0.307003915309906 Acc: 0.8515625\n",
      "Epoch Step: 2000 Loss: 0.33969852328300476 Acc: 0.8515625\n",
      "Epoch Step: 2200 Loss: 0.32640814781188965 Acc: 0.875\n",
      "Epoch Step: 2400 Loss: 0.32653966546058655 Acc: 0.859375\n",
      "Epoch Step: 2600 Loss: 0.4190340042114258 Acc: 0.8203125\n",
      "Epoch Step: 2800 Loss: 0.3021368086338043 Acc: 0.875\n",
      "Epoch Step: 3000 Loss: 0.3446864187717438 Acc: 0.828125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5071722848603049, model saved\n",
      "\n",
      "Epoch 10, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.2910562753677368 Acc: 0.8828125\n",
      "Epoch Step: 200 Loss: 0.20006360113620758 Acc: 0.921875\n",
      "Epoch Step: 400 Loss: 0.18826574087142944 Acc: 0.890625\n",
      "Epoch Step: 600 Loss: 0.17119865119457245 Acc: 0.921875\n",
      "Epoch Step: 800 Loss: 0.2144201099872589 Acc: 0.9140625\n",
      "Epoch Step: 1000 Loss: 0.18615247309207916 Acc: 0.8984375\n",
      "Epoch Step: 1200 Loss: 0.3275305926799774 Acc: 0.828125\n",
      "Epoch Step: 1400 Loss: 0.322292685508728 Acc: 0.875\n",
      "Epoch Step: 1600 Loss: 0.3341027498245239 Acc: 0.8515625\n",
      "Epoch Step: 1800 Loss: 0.24686171114444733 Acc: 0.875\n",
      "Epoch Step: 2000 Loss: 0.22115415334701538 Acc: 0.8984375\n",
      "Epoch Step: 2200 Loss: 0.22448110580444336 Acc: 0.8984375\n",
      "Epoch Step: 2400 Loss: 0.2686280608177185 Acc: 0.8828125\n",
      "Epoch Step: 2600 Loss: 0.3023141324520111 Acc: 0.8359375\n",
      "Epoch Step: 2800 Loss: 0.34167537093162537 Acc: 0.8515625\n",
      "Epoch Step: 3000 Loss: 0.2920741140842438 Acc: 0.8671875\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5002942771744775, model saved\n",
      "\n",
      "Epoch 11, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.16623687744140625 Acc: 0.9375\n",
      "Epoch Step: 200 Loss: 0.1506398469209671 Acc: 0.9375\n",
      "Epoch Step: 400 Loss: 0.2585093379020691 Acc: 0.8984375\n",
      "Epoch Step: 600 Loss: 0.16114522516727448 Acc: 0.9296875\n",
      "Epoch Step: 800 Loss: 0.19759108126163483 Acc: 0.9140625\n",
      "Epoch Step: 1000 Loss: 0.2014601081609726 Acc: 0.9375\n",
      "Epoch Step: 1200 Loss: 0.28015217185020447 Acc: 0.890625\n",
      "Epoch Step: 1400 Loss: 0.22335165739059448 Acc: 0.8984375\n",
      "Epoch Step: 1600 Loss: 0.2387525886297226 Acc: 0.8984375\n",
      "Epoch Step: 1800 Loss: 0.3109138309955597 Acc: 0.890625\n",
      "Epoch Step: 2000 Loss: 0.21979530155658722 Acc: 0.875\n",
      "Epoch Step: 2200 Loss: 0.2035709023475647 Acc: 0.921875\n",
      "Epoch Step: 2400 Loss: 0.23636889457702637 Acc: 0.90625\n",
      "Epoch Step: 2600 Loss: 0.2391536682844162 Acc: 0.890625\n",
      "Epoch Step: 2800 Loss: 0.31457117199897766 Acc: 0.8515625\n",
      "Epoch Step: 3000 Loss: 0.20847460627555847 Acc: 0.890625\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5028215987905785, model saved\n",
      "\n",
      "Epoch 12, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.16343463957309723 Acc: 0.90625\n",
      "Epoch Step: 200 Loss: 0.1971377730369568 Acc: 0.921875\n",
      "Epoch Step: 400 Loss: 0.1348683089017868 Acc: 0.9296875\n",
      "Epoch Step: 600 Loss: 0.1608765572309494 Acc: 0.953125\n",
      "Epoch Step: 800 Loss: 0.1738876849412918 Acc: 0.921875\n",
      "Epoch Step: 1000 Loss: 0.1737745851278305 Acc: 0.921875\n",
      "Epoch Step: 1200 Loss: 0.27747610211372375 Acc: 0.9140625\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, classifier_model, criterion, optimizer, classifier_num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 0 Acc: 0.953125\n",
      "Epoch Step: 1 Acc: 0.9609375\n",
      "Epoch Step: 2 Acc: 0.9921875\n",
      "Epoch Step: 3 Acc: 0.984375\n",
      "Epoch Step: 4 Acc: 0.9453125\n",
      "Epoch Step: 5 Acc: 0.9609375\n",
      "Epoch Step: 6 Acc: 0.96875\n",
      "Epoch Step: 7 Acc: 0.90625\n",
      "Epoch Step: 8 Acc: 0.9609375\n",
      "Epoch Step: 9 Acc: 0.9609375\n",
      "Epoch Step: 10 Acc: 0.9609375\n",
      "Epoch Step: 11 Acc: 0.953125\n",
      "Epoch Step: 12 Acc: 0.96875\n",
      "Epoch Step: 13 Acc: 0.96875\n",
      "Epoch Step: 14 Acc: 0.96875\n",
      "Epoch Step: 15 Acc: 1.0\n",
      "Epoch Step: 16 Acc: 0.96875\n",
      "Epoch Step: 17 Acc: 0.9921875\n",
      "Epoch Step: 18 Acc: 0.96875\n",
      "Epoch Step: 19 Acc: 0.9453125\n",
      "Epoch Step: 20 Acc: 0.9765625\n",
      "Epoch Step: 21 Acc: 0.9765625\n",
      "Epoch Step: 22 Acc: 0.96875\n",
      "Epoch Step: 23 Acc: 0.9609375\n",
      "Epoch Step: 24 Acc: 0.96875\n",
      "Epoch Step: 25 Acc: 0.96875\n",
      "Epoch Step: 26 Acc: 0.9765625\n",
      "Epoch Step: 27 Acc: 0.984375\n",
      "Epoch Step: 28 Acc: 0.9609375\n",
      "Epoch Step: 29 Acc: 0.953125\n",
      "Epoch Step: 30 Acc: 0.953125\n",
      "Epoch Step: 31 Acc: 0.953125\n",
      "Epoch Step: 32 Acc: 0.9609375\n",
      "Epoch Step: 33 Acc: 0.9609375\n",
      "Epoch Step: 34 Acc: 0.9765625\n",
      "Epoch Step: 35 Acc: 0.9375\n",
      "Epoch Step: 36 Acc: 0.9609375\n",
      "Epoch Step: 37 Acc: 0.9765625\n",
      "Epoch Step: 38 Acc: 0.96875\n",
      "Epoch Step: 39 Acc: 0.9296875\n",
      "Epoch Step: 40 Acc: 0.9765625\n",
      "Epoch Step: 41 Acc: 0.9765625\n",
      "Epoch Step: 42 Acc: 0.9453125\n",
      "Epoch Step: 43 Acc: 0.9609375\n",
      "Epoch Step: 44 Acc: 1.0\n",
      "Epoch Step: 45 Acc: 0.984375\n",
      "Epoch Step: 46 Acc: 0.9921875\n",
      "Epoch Step: 47 Acc: 0.9296875\n",
      "Epoch Step: 48 Acc: 0.953125\n",
      "Epoch Step: 49 Acc: 0.9453125\n",
      "Epoch Step: 50 Acc: 0.921875\n",
      "Epoch Step: 51 Acc: 0.984375\n",
      "Epoch Step: 52 Acc: 0.96875\n",
      "Epoch Step: 53 Acc: 0.96875\n",
      "Epoch Step: 54 Acc: 0.9609375\n",
      "Epoch Step: 55 Acc: 0.9609375\n",
      "Epoch Step: 56 Acc: 0.9609375\n",
      "Epoch Step: 57 Acc: 0.953125\n",
      "Epoch Step: 58 Acc: 0.9453125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m src_ids \u001b[39m=\u001b[39m src_ids\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m tgt \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m logits \u001b[39m=\u001b[39m classifier_model(src_ids, src_len)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch Step: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m Acc: \u001b[39m\u001b[39m{\u001b[39;00m(logits\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m tgt)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\56830\\OneDrive - Massachusetts Institute of Technology\\Course Files\\6.8611\\dedeos-project\\models\\classifier_lstm.py:32\u001b[0m, in \u001b[0;36mClassifierLSTM.forward\u001b[1;34m(self, src_ids, src_len)\u001b[0m\n\u001b[0;32m     30\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(src_ids)\n\u001b[0;32m     31\u001b[0m packed_src \u001b[39m=\u001b[39m pack_padded_sequence(embed, src_len, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m _, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(packed_src)\n\u001b[0;32m     33\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[0;32m     34\u001b[0m     hidden\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(hidden\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \n\u001b[0;32m     35\u001b[0m     cell\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(cell\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m     ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)\n",
      "File \u001b[1;32mc:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[0;32m    774\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m    775\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch, (src_ids, src_len, tgt) in enumerate(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        src_ids = src_ids.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        logits = classifier_model(src_ids, src_len)\n",
    "        print(f\"Epoch Step: {batch} Acc: {(logits.argmax(1) == tgt).sum().item() / tgt.size(0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ecdeeea834996ab17c476b9c82385c086ccb21d059ebd3afa1913627f92baf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
