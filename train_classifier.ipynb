{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from load_data import Tokenizer, GenderDataset, gender_data_collate_fn\n",
    "from models.classifier_lstm import ClassifierLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_embedding_size = 512\n",
    "classifier_hidden_size = 512\n",
    "classifier_num_layers = 2\n",
    "classifier_is_bidirectional = True\n",
    "\n",
    "classifier_lr = 3e-4\n",
    "classifier_num_epoch = 100\n",
    "classifier_batch_size = 128\n",
    "classifier_max_norm = 2\n",
    "\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, \"data\", \"blog.json\"), \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "docs = json_data['docs'][1:] # I don't want to see the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19676/19676 [00:34<00:00, 576.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 559126\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 559126/559126 [00:16<00:00, 32907.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 554016\n",
      "Number of unique words before converting to <UNK>:  505954\n",
      "Converting words with frequencies less than 10 to <UNK>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "554016it [00:12, 45847.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting <UNK>:  59178\n",
      "Known occurrences rate 98.69%\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13773 2951 2952\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(docs)\n",
    "num_train_docs = int(num_docs * 0.7)\n",
    "num_val_docs = int(num_docs * 0.15)\n",
    "num_test_docs = num_docs - num_train_docs - num_val_docs\n",
    "print(num_train_docs, num_val_docs, num_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:num_train_docs]\n",
    "val_docs = docs[num_train_docs:num_train_docs+num_val_docs]\n",
    "test_docs = docs[num_train_docs+num_val_docs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13773/13773 [00:24<00:00, 563.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 405965\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405965/405965 [00:11<00:00, 35529.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 402316\n",
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2951/2951 [00:05<00:00, 566.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 87422\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87422/87422 [00:02<00:00, 34650.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 86653\n",
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2952/2952 [00:03<00:00, 764.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 65739\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65739/65739 [00:02<00:00, 32005.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 65047\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GenderDataset(train_docs, tokenizer)\n",
    "val_dataset = GenderDataset(val_docs, tokenizer)\n",
    "test_dataset = GenderDataset(test_docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([30275, 52036, 14412, 2662, 49226, 10062, 11762, 20664, 6772, 52364, 48343, 10062, 49309, 33403, 38804, 52036, 53265, 10062, 24796, 49226, 37486, 38804, 37486, 56870, 7998, 12358, 37486, 21323, 30832, 24785, 37486, 1530, 30556, 38804, 20065, 38804, 28927, 52036, 4514, 56936, 42360, 14412, 42607, 42719, 10062, 50735, 49226, 10062, 56936, 51328, 38804, 52036, 56632, 24820, 56802, 14412, 10062, 33186, 19301, 42719, 1078, 16301, 38804, 24785, 18994, 14412, 11371, 1530, 27207, 1530, 466, 44896, 52036, 1596, 24785, 19945, 24820, 28853, 466, 49411, 38804, 48343, 51790, 21323, 10062, 25057, 32466, 48343, 51170, 33597, 34179, 45849, 57746, 52747, 57074, 34356, 24785, 49144, 38804, 7362], 0) ([30275, 10349, 41147, 46695, 5492, 9836, 41756, 44196, 21323, 1280, 44120, 47077, 20325, 4688, 43985, 42719, 57023, 34415, 56452, 21323, 24785, 10349, 57673, 57074, 19169, 23810, 52036, 42719, 41869, 16639, 38804, 55987, 10349, 39996, 9343, 48353, 48343, 10062, 46818, 51617, 49116, 50723, 55151, 1280, 25875, 14706, 53807, 42719, 23810, 48745, 33926, 42719, 41869, 48126, 55987, 10349, 1474, 10298, 8211, 38804, 10349, 54006, 29133, 32207, 57673, 47229, 25631, 24785, 2603, 45849, 32185, 44371, 46695, 21323, 28853, 38696, 6866, 39083, 3899, 79, 30174, 38804, 41780, 10349, 54006, 23810, 11716, 42719, 10062, 37486, 10349, 39996, 1280, 27703, 47077, 37993, 24785, 10349, 10298, 48343, 21323, 52036, 43985, 41462, 46502, 55987, 10062, 49116, 50723, 47008, 26372, 52273, 24785, 48745, 44938, 319, 26513, 54032, 48343, 10062, 58133, 24785, 2654, 19169, 41869, 40573, 38804, 7362], 1) ([30275, 10349, 24979, 36073, 49622, 45849, 3367, 31679, 24820, 14362, 11959, 29133, 48343, 43499, 49356, 466, 19247, 53734, 48745, 3367, 31679, 53960, 466, 1474, 25469, 48745, 46165, 10349, 8243, 25001, 56591, 24785, 5906, 24785, 26372, 25001, 36472, 45131, 24785, 20479, 41780, 32, 1936, 40548, 48745, 29151, 24006, 466, 47650, 17795, 42719, 27578, 10062, 41533, 38804, 48353, 29590, 55987, 46131, 10349, 2654, 8885, 37423, 21323, 14867, 53960, 10349, 53532, 21133, 19442, 46695, 48353, 29590, 12997, 30174, 49226, 14199, 10349, 1547, 10349, 7909, 16108, 3899, 34146, 29590, 10062, 4087, 49226, 12997, 38629, 53960, 52036, 18522, 10062, 3367, 1065, 21323, 613, 38804, 10349, 53532, 24382, 34016, 10859, 58606, 49226, 3367, 39205, 55283, 52937, 48369, 42040, 14867, 21323, 1474, 10349, 33014, 12997, 38804, 14044, 7073, 26372, 51340, 29133, 38804, 7362], 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1], val_dataset[1], test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=classifier_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=gender_data_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = ClassifierLSTM(\n",
    "    tokenizer.vocab_size(), \n",
    "    classifier_embedding_size, \n",
    "    classifier_hidden_size, \n",
    "    classifier_num_layers, \n",
    "    classifier_is_bidirectional\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier_model.parameters(), lr = classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, criterion, optimizer, num_epoch):\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(f\"Epoch {epoch}, total {len(train_dataloader)} batches\\n\")\n",
    "        model.train()\n",
    "\n",
    "        for batch, (src_ids, src_len, tgt) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            src_ids = src_ids.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            logits = model(src_ids, src_len)\n",
    "            loss = criterion(logits, tgt)\n",
    "            if batch % print_every == 0:\n",
    "                print(f\"Epoch Step: {batch} Loss: {loss} Acc: {(logits.argmax(1) == tgt).sum().item() / tgt.size(0)}\")\n",
    "\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), classifier_max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\nBegin Evaluation\")\n",
    "        model.eval()\n",
    "        total_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (src_ids, src_len, tgt) in enumerate(val_dataloader):\n",
    "                src_ids = src_ids.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                logits = model(src_ids, src_len)\n",
    "                total_acc += (logits.argmax(1) == tgt).sum().item()\n",
    "        \n",
    "        acc = total_acc / len(val_dataloader.dataset)\n",
    "        print(f\"Validation Accuracy: {acc}, model saved\\n\")\n",
    "\n",
    "        torch.save(model.state_dict(), f'./save/classifier_model_{classifier_hidden_size}_{classifier_batch_size}_{classifier_num_layers}_{classifier_is_bidirectional}_epoch_{epoch}.file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6901217699050903 Acc: 0.515625\n",
      "Epoch Step: 200 Loss: 0.6968944668769836 Acc: 0.515625\n",
      "Epoch Step: 400 Loss: 0.6965958476066589 Acc: 0.515625\n",
      "Epoch Step: 600 Loss: 0.691948413848877 Acc: 0.5390625\n",
      "Epoch Step: 800 Loss: 0.6927976012229919 Acc: 0.5234375\n",
      "Epoch Step: 1000 Loss: 0.7107475399971008 Acc: 0.4375\n",
      "Epoch Step: 1200 Loss: 0.6919336318969727 Acc: 0.53125\n",
      "Epoch Step: 1400 Loss: 0.6899346113204956 Acc: 0.53125\n",
      "Epoch Step: 1600 Loss: 0.68748539686203 Acc: 0.546875\n",
      "Epoch Step: 1800 Loss: 0.6952205896377563 Acc: 0.515625\n",
      "Epoch Step: 2000 Loss: 0.6928135752677917 Acc: 0.53125\n",
      "Epoch Step: 2200 Loss: 0.6911121606826782 Acc: 0.546875\n",
      "Epoch Step: 2400 Loss: 0.6961835622787476 Acc: 0.4921875\n",
      "Epoch Step: 2600 Loss: 0.6776772737503052 Acc: 0.578125\n",
      "Epoch Step: 2800 Loss: 0.6839342713356018 Acc: 0.5703125\n",
      "Epoch Step: 3000 Loss: 0.6882370114326477 Acc: 0.53125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5340726806919552, model saved\n",
      "\n",
      "Epoch 1, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6896440982818604 Acc: 0.5390625\n",
      "Epoch Step: 200 Loss: 0.6936426162719727 Acc: 0.546875\n",
      "Epoch Step: 400 Loss: 0.6884227991104126 Acc: 0.5546875\n",
      "Epoch Step: 600 Loss: 0.6825854182243347 Acc: 0.5703125\n",
      "Epoch Step: 800 Loss: 0.6948641538619995 Acc: 0.46875\n",
      "Epoch Step: 1000 Loss: 0.6701480746269226 Acc: 0.6328125\n",
      "Epoch Step: 1200 Loss: 0.6880083084106445 Acc: 0.546875\n",
      "Epoch Step: 1400 Loss: 0.689495861530304 Acc: 0.515625\n",
      "Epoch Step: 1600 Loss: 0.6858367323875427 Acc: 0.5625\n",
      "Epoch Step: 1800 Loss: 0.6893024444580078 Acc: 0.5625\n",
      "Epoch Step: 2000 Loss: 0.6687841415405273 Acc: 0.6484375\n",
      "Epoch Step: 2200 Loss: 0.6830340623855591 Acc: 0.5859375\n",
      "Epoch Step: 2400 Loss: 0.6920863389968872 Acc: 0.515625\n",
      "Epoch Step: 2600 Loss: 0.68257075548172 Acc: 0.546875\n",
      "Epoch Step: 2800 Loss: 0.6941837072372437 Acc: 0.5234375\n",
      "Epoch Step: 3000 Loss: 0.6927393674850464 Acc: 0.5234375\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5286487484564873, model saved\n",
      "\n",
      "Epoch 2, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.7036367654800415 Acc: 0.5390625\n",
      "Epoch Step: 200 Loss: 0.6719599962234497 Acc: 0.5390625\n",
      "Epoch Step: 400 Loss: 0.6634323596954346 Acc: 0.6015625\n",
      "Epoch Step: 600 Loss: 0.6718156337738037 Acc: 0.546875\n",
      "Epoch Step: 800 Loss: 0.6966999173164368 Acc: 0.59375\n",
      "Epoch Step: 1000 Loss: 0.6742495894432068 Acc: 0.5390625\n",
      "Epoch Step: 1200 Loss: 0.6848863959312439 Acc: 0.5390625\n",
      "Epoch Step: 1400 Loss: 0.6577043533325195 Acc: 0.5859375\n",
      "Epoch Step: 1600 Loss: 0.665385365486145 Acc: 0.5546875\n",
      "Epoch Step: 1800 Loss: 0.6489631533622742 Acc: 0.6953125\n",
      "Epoch Step: 2000 Loss: 0.6713951826095581 Acc: 0.625\n",
      "Epoch Step: 2200 Loss: 0.6745244264602661 Acc: 0.6015625\n",
      "Epoch Step: 2400 Loss: 0.675132155418396 Acc: 0.578125\n",
      "Epoch Step: 2600 Loss: 0.670635998249054 Acc: 0.6015625\n",
      "Epoch Step: 2800 Loss: 0.6915643811225891 Acc: 0.5\n",
      "Epoch Step: 3000 Loss: 0.6506369709968567 Acc: 0.65625\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5132424728514882, model saved\n",
      "\n",
      "Epoch 3, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.6323466897010803 Acc: 0.6796875\n",
      "Epoch Step: 200 Loss: 0.6147124767303467 Acc: 0.6796875\n",
      "Epoch Step: 400 Loss: 0.6075500845909119 Acc: 0.6796875\n",
      "Epoch Step: 600 Loss: 0.6331895589828491 Acc: 0.578125\n",
      "Epoch Step: 800 Loss: 0.6361883878707886 Acc: 0.6484375\n",
      "Epoch Step: 1000 Loss: 0.6016707420349121 Acc: 0.6640625\n",
      "Epoch Step: 1200 Loss: 0.5993072986602783 Acc: 0.6640625\n",
      "Epoch Step: 1400 Loss: 0.6417447924613953 Acc: 0.6328125\n",
      "Epoch Step: 1600 Loss: 0.6296749711036682 Acc: 0.6015625\n",
      "Epoch Step: 1800 Loss: 0.6596454977989197 Acc: 0.609375\n",
      "Epoch Step: 2000 Loss: 0.6320117712020874 Acc: 0.640625\n",
      "Epoch Step: 2200 Loss: 0.6399509906768799 Acc: 0.6328125\n",
      "Epoch Step: 2400 Loss: 0.6231207251548767 Acc: 0.65625\n",
      "Epoch Step: 2600 Loss: 0.6283267736434937 Acc: 0.6484375\n",
      "Epoch Step: 2800 Loss: 0.6344544291496277 Acc: 0.625\n",
      "Epoch Step: 3000 Loss: 0.6100506782531738 Acc: 0.671875\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5072761473924734, model saved\n",
      "\n",
      "Epoch 4, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.540148138999939 Acc: 0.71875\n",
      "Epoch Step: 200 Loss: 0.5139782428741455 Acc: 0.75\n",
      "Epoch Step: 400 Loss: 0.49748045206069946 Acc: 0.75\n",
      "Epoch Step: 600 Loss: 0.5502287149429321 Acc: 0.6875\n",
      "Epoch Step: 800 Loss: 0.499843031167984 Acc: 0.765625\n",
      "Epoch Step: 1000 Loss: 0.4810420274734497 Acc: 0.75\n",
      "Epoch Step: 1200 Loss: 0.5210030674934387 Acc: 0.7421875\n",
      "Epoch Step: 1400 Loss: 0.5413592457771301 Acc: 0.71875\n",
      "Epoch Step: 1600 Loss: 0.5281150341033936 Acc: 0.7421875\n",
      "Epoch Step: 1800 Loss: 0.4893958866596222 Acc: 0.734375\n",
      "Epoch Step: 2000 Loss: 0.49873796105384827 Acc: 0.7421875\n",
      "Epoch Step: 2200 Loss: 0.6171222925186157 Acc: 0.671875\n",
      "Epoch Step: 2400 Loss: 0.5424392223358154 Acc: 0.6875\n",
      "Epoch Step: 2600 Loss: 0.47995325922966003 Acc: 0.7578125\n",
      "Epoch Step: 2800 Loss: 0.5505605340003967 Acc: 0.7421875\n",
      "Epoch Step: 3000 Loss: 0.5541083216667175 Acc: 0.703125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5035486365157582, model saved\n",
      "\n",
      "Epoch 5, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.38074856996536255 Acc: 0.8671875\n",
      "Epoch Step: 200 Loss: 0.34093618392944336 Acc: 0.8828125\n",
      "Epoch Step: 400 Loss: 0.363028883934021 Acc: 0.8359375\n",
      "Epoch Step: 600 Loss: 0.3476797044277191 Acc: 0.84375\n",
      "Epoch Step: 800 Loss: 0.3190758228302002 Acc: 0.8671875\n",
      "Epoch Step: 1000 Loss: 0.31653130054473877 Acc: 0.875\n",
      "Epoch Step: 1200 Loss: 0.37264159321784973 Acc: 0.8046875\n",
      "Epoch Step: 1400 Loss: 0.36201152205467224 Acc: 0.84375\n",
      "Epoch Step: 1600 Loss: 0.2919226586818695 Acc: 0.8828125\n",
      "Epoch Step: 1800 Loss: 0.3956121802330017 Acc: 0.8203125\n",
      "Epoch Step: 2000 Loss: 0.36418354511260986 Acc: 0.828125\n",
      "Epoch Step: 2200 Loss: 0.2811312973499298 Acc: 0.890625\n",
      "Epoch Step: 2400 Loss: 0.4130972623825073 Acc: 0.78125\n",
      "Epoch Step: 2600 Loss: 0.3688317835330963 Acc: 0.8359375\n",
      "Epoch Step: 2800 Loss: 0.3877708315849304 Acc: 0.8671875\n",
      "Epoch Step: 3000 Loss: 0.36375075578689575 Acc: 0.828125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5005597036455749, model saved\n",
      "\n",
      "Epoch 6, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.21848753094673157 Acc: 0.9609375\n",
      "Epoch Step: 200 Loss: 0.1801804155111313 Acc: 0.9140625\n",
      "Epoch Step: 400 Loss: 0.23297400772571564 Acc: 0.9140625\n",
      "Epoch Step: 600 Loss: 0.33234184980392456 Acc: 0.8828125\n",
      "Epoch Step: 800 Loss: 0.23859895765781403 Acc: 0.890625\n",
      "Epoch Step: 1000 Loss: 0.25343960523605347 Acc: 0.921875\n",
      "Epoch Step: 1200 Loss: 0.24707041680812836 Acc: 0.875\n",
      "Epoch Step: 1400 Loss: 0.187200129032135 Acc: 0.90625\n",
      "Epoch Step: 1600 Loss: 0.22792936861515045 Acc: 0.8984375\n",
      "Epoch Step: 1800 Loss: 0.14071325957775116 Acc: 0.953125\n",
      "Epoch Step: 2000 Loss: 0.23190230131149292 Acc: 0.90625\n",
      "Epoch Step: 2200 Loss: 0.21357496082782745 Acc: 0.9296875\n",
      "Epoch Step: 2400 Loss: 0.15700465440750122 Acc: 0.953125\n",
      "Epoch Step: 2600 Loss: 0.2653079926967621 Acc: 0.890625\n",
      "Epoch Step: 2800 Loss: 0.16936182975769043 Acc: 0.9296875\n",
      "Epoch Step: 3000 Loss: 0.25558948516845703 Acc: 0.8828125\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.5026715751330018, model saved\n",
      "\n",
      "Epoch 7, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.11554110795259476 Acc: 0.9765625\n",
      "Epoch Step: 200 Loss: 0.1741449534893036 Acc: 0.9453125\n",
      "Epoch Step: 400 Loss: 0.06883096694946289 Acc: 0.9765625\n",
      "Epoch Step: 600 Loss: 0.1324126273393631 Acc: 0.96875\n",
      "Epoch Step: 800 Loss: 0.05610409751534462 Acc: 0.984375\n",
      "Epoch Step: 1000 Loss: 0.16078521311283112 Acc: 0.9609375\n",
      "Epoch Step: 1200 Loss: 0.20322397351264954 Acc: 0.9453125\n",
      "Epoch Step: 1400 Loss: 0.10338468104600906 Acc: 0.984375\n",
      "Epoch Step: 1600 Loss: 0.19157420098781586 Acc: 0.921875\n",
      "Epoch Step: 1800 Loss: 0.19509011507034302 Acc: 0.9453125\n",
      "Epoch Step: 2000 Loss: 0.18159236013889313 Acc: 0.9609375\n",
      "Epoch Step: 2200 Loss: 0.14911334216594696 Acc: 0.9296875\n",
      "Epoch Step: 2400 Loss: 0.20480462908744812 Acc: 0.9296875\n",
      "Epoch Step: 2600 Loss: 0.16106173396110535 Acc: 0.9296875\n",
      "Epoch Step: 2800 Loss: 0.1603873074054718 Acc: 0.953125\n",
      "Epoch Step: 3000 Loss: 0.1392204761505127 Acc: 0.9375\n",
      "\n",
      "Begin Evaluation\n",
      "Validation Accuracy: 0.49998268957797193, model saved\n",
      "\n",
      "Epoch 8, total 3144 batches\n",
      "\n",
      "Epoch Step: 0 Loss: 0.1341245174407959 Acc: 0.9453125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(train_dataloader, val_dataloader, classifier_model, criterion, optimizer, classifier_num_epoch)\n",
      "Cell \u001b[1;32mIn [55], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, val_dataloader, model, criterion, optimizer, num_epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch Step: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m Acc: \u001b[39m\u001b[39m{\u001b[39;00m(logits\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m tgt)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     19\u001b[0m \u001b[39m# nn.utils.clip_grad_norm_(model.parameters(), classifier_max_norm)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\56830\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, classifier_model, criterion, optimizer, classifier_num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = ClassifierLSTM(\n",
    "    tokenizer.vocab_size(), \n",
    "    classifier_embedding_size, \n",
    "    classifier_hidden_size, \n",
    "    classifier_num_layers, \n",
    "    classifier_is_bidirectional\n",
    ").to(device)\n",
    "trained_model.load_state_dict(torch.load(\"./save/classifier_model_512_128_2_True_epoch_7.file\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 0 Acc: 0.5625\n",
      "Epoch Step: 1 Acc: 0.453125\n",
      "Epoch Step: 2 Acc: 0.4453125\n",
      "Epoch Step: 3 Acc: 0.5078125\n",
      "Epoch Step: 4 Acc: 0.5\n",
      "Epoch Step: 5 Acc: 0.4765625\n",
      "Epoch Step: 6 Acc: 0.484375\n",
      "Epoch Step: 7 Acc: 0.453125\n",
      "Epoch Step: 8 Acc: 0.4765625\n",
      "Epoch Step: 9 Acc: 0.4453125\n",
      "Epoch Step: 10 Acc: 0.40625\n",
      "Epoch Step: 11 Acc: 0.4296875\n",
      "Epoch Step: 12 Acc: 0.421875\n",
      "Epoch Step: 13 Acc: 0.4375\n",
      "Epoch Step: 14 Acc: 0.5078125\n",
      "Epoch Step: 15 Acc: 0.4765625\n",
      "Epoch Step: 16 Acc: 0.4921875\n",
      "Epoch Step: 17 Acc: 0.53125\n",
      "Epoch Step: 18 Acc: 0.4453125\n",
      "Epoch Step: 19 Acc: 0.5390625\n",
      "Epoch Step: 20 Acc: 0.46875\n",
      "Epoch Step: 21 Acc: 0.5234375\n",
      "Epoch Step: 22 Acc: 0.4765625\n",
      "Epoch Step: 23 Acc: 0.5234375\n",
      "Epoch Step: 24 Acc: 0.5859375\n",
      "Epoch Step: 25 Acc: 0.4765625\n",
      "Epoch Step: 26 Acc: 0.4921875\n",
      "Epoch Step: 27 Acc: 0.453125\n",
      "Epoch Step: 28 Acc: 0.5\n",
      "Epoch Step: 29 Acc: 0.4140625\n",
      "Epoch Step: 30 Acc: 0.40625\n",
      "Epoch Step: 31 Acc: 0.5078125\n",
      "Epoch Step: 32 Acc: 0.453125\n",
      "Epoch Step: 33 Acc: 0.4609375\n",
      "Epoch Step: 34 Acc: 0.453125\n",
      "Epoch Step: 35 Acc: 0.5234375\n",
      "Epoch Step: 36 Acc: 0.5078125\n",
      "Epoch Step: 37 Acc: 0.4921875\n",
      "Epoch Step: 38 Acc: 0.4921875\n",
      "Epoch Step: 39 Acc: 0.484375\n",
      "Epoch Step: 40 Acc: 0.515625\n",
      "Epoch Step: 41 Acc: 0.515625\n",
      "Epoch Step: 42 Acc: 0.4609375\n",
      "Epoch Step: 43 Acc: 0.5546875\n",
      "Epoch Step: 44 Acc: 0.53125\n",
      "Epoch Step: 45 Acc: 0.3671875\n",
      "Epoch Step: 46 Acc: 0.484375\n",
      "Epoch Step: 47 Acc: 0.484375\n",
      "Epoch Step: 48 Acc: 0.5078125\n",
      "Epoch Step: 49 Acc: 0.453125\n",
      "Epoch Step: 50 Acc: 0.4375\n",
      "Epoch Step: 51 Acc: 0.4609375\n",
      "Epoch Step: 52 Acc: 0.5\n",
      "Epoch Step: 53 Acc: 0.5078125\n",
      "Epoch Step: 54 Acc: 0.453125\n",
      "Epoch Step: 55 Acc: 0.53125\n",
      "Epoch Step: 56 Acc: 0.5390625\n",
      "Epoch Step: 57 Acc: 0.4375\n",
      "Epoch Step: 58 Acc: 0.53125\n",
      "Epoch Step: 59 Acc: 0.515625\n",
      "Epoch Step: 60 Acc: 0.484375\n",
      "Epoch Step: 61 Acc: 0.453125\n",
      "Epoch Step: 62 Acc: 0.453125\n",
      "Epoch Step: 63 Acc: 0.53125\n",
      "Epoch Step: 64 Acc: 0.4453125\n",
      "Epoch Step: 65 Acc: 0.4453125\n",
      "Epoch Step: 66 Acc: 0.4296875\n",
      "Epoch Step: 67 Acc: 0.4453125\n",
      "Epoch Step: 68 Acc: 0.4921875\n",
      "Epoch Step: 69 Acc: 0.484375\n",
      "Epoch Step: 70 Acc: 0.4609375\n",
      "Epoch Step: 71 Acc: 0.484375\n",
      "Epoch Step: 72 Acc: 0.515625\n",
      "Epoch Step: 73 Acc: 0.515625\n",
      "Epoch Step: 74 Acc: 0.4765625\n",
      "Epoch Step: 75 Acc: 0.578125\n",
      "Epoch Step: 76 Acc: 0.4921875\n",
      "Epoch Step: 77 Acc: 0.484375\n",
      "Epoch Step: 78 Acc: 0.5\n",
      "Epoch Step: 79 Acc: 0.453125\n",
      "Epoch Step: 80 Acc: 0.4765625\n",
      "Epoch Step: 81 Acc: 0.46875\n",
      "Epoch Step: 82 Acc: 0.484375\n",
      "Epoch Step: 83 Acc: 0.5390625\n",
      "Epoch Step: 84 Acc: 0.484375\n",
      "Epoch Step: 85 Acc: 0.484375\n",
      "Epoch Step: 86 Acc: 0.5625\n",
      "Epoch Step: 87 Acc: 0.5078125\n",
      "Epoch Step: 88 Acc: 0.40625\n",
      "Epoch Step: 89 Acc: 0.5390625\n",
      "Epoch Step: 90 Acc: 0.5\n",
      "Epoch Step: 91 Acc: 0.53125\n",
      "Epoch Step: 92 Acc: 0.53125\n",
      "Epoch Step: 93 Acc: 0.46875\n",
      "Epoch Step: 94 Acc: 0.4765625\n",
      "Epoch Step: 95 Acc: 0.5234375\n",
      "Epoch Step: 96 Acc: 0.453125\n",
      "Epoch Step: 97 Acc: 0.484375\n",
      "Epoch Step: 98 Acc: 0.46875\n",
      "Epoch Step: 99 Acc: 0.40625\n",
      "Epoch Step: 100 Acc: 0.5078125\n",
      "Epoch Step: 101 Acc: 0.5234375\n",
      "Epoch Step: 102 Acc: 0.5234375\n",
      "Epoch Step: 103 Acc: 0.5234375\n",
      "Epoch Step: 104 Acc: 0.53125\n",
      "Epoch Step: 105 Acc: 0.5078125\n",
      "Epoch Step: 106 Acc: 0.5234375\n",
      "Epoch Step: 107 Acc: 0.453125\n",
      "Epoch Step: 108 Acc: 0.5\n",
      "Epoch Step: 109 Acc: 0.4609375\n",
      "Epoch Step: 110 Acc: 0.5078125\n",
      "Epoch Step: 111 Acc: 0.53125\n",
      "Epoch Step: 112 Acc: 0.5625\n",
      "Epoch Step: 113 Acc: 0.5234375\n",
      "Epoch Step: 114 Acc: 0.453125\n",
      "Epoch Step: 115 Acc: 0.4921875\n",
      "Epoch Step: 116 Acc: 0.53125\n",
      "Epoch Step: 117 Acc: 0.453125\n",
      "Epoch Step: 118 Acc: 0.515625\n",
      "Epoch Step: 119 Acc: 0.5078125\n",
      "Epoch Step: 120 Acc: 0.515625\n",
      "Epoch Step: 121 Acc: 0.4140625\n",
      "Epoch Step: 122 Acc: 0.53125\n",
      "Epoch Step: 123 Acc: 0.5\n",
      "Epoch Step: 124 Acc: 0.4296875\n",
      "Epoch Step: 125 Acc: 0.515625\n",
      "Epoch Step: 126 Acc: 0.4296875\n",
      "Epoch Step: 127 Acc: 0.5234375\n",
      "Epoch Step: 128 Acc: 0.3828125\n",
      "Epoch Step: 129 Acc: 0.53125\n",
      "Epoch Step: 130 Acc: 0.6015625\n",
      "Epoch Step: 131 Acc: 0.5\n",
      "Epoch Step: 132 Acc: 0.515625\n",
      "Epoch Step: 133 Acc: 0.4296875\n",
      "Epoch Step: 134 Acc: 0.515625\n",
      "Epoch Step: 135 Acc: 0.5234375\n",
      "Epoch Step: 136 Acc: 0.609375\n",
      "Epoch Step: 137 Acc: 0.4453125\n",
      "Epoch Step: 138 Acc: 0.4453125\n",
      "Epoch Step: 139 Acc: 0.5546875\n",
      "Epoch Step: 140 Acc: 0.40625\n",
      "Epoch Step: 141 Acc: 0.4375\n",
      "Epoch Step: 142 Acc: 0.484375\n",
      "Epoch Step: 143 Acc: 0.484375\n",
      "Epoch Step: 144 Acc: 0.4609375\n",
      "Epoch Step: 145 Acc: 0.515625\n",
      "Epoch Step: 146 Acc: 0.40625\n",
      "Epoch Step: 147 Acc: 0.5234375\n",
      "Epoch Step: 148 Acc: 0.4453125\n",
      "Epoch Step: 149 Acc: 0.484375\n",
      "Epoch Step: 150 Acc: 0.46875\n",
      "Epoch Step: 151 Acc: 0.5625\n",
      "Epoch Step: 152 Acc: 0.5078125\n",
      "Epoch Step: 153 Acc: 0.5546875\n",
      "Epoch Step: 154 Acc: 0.5\n",
      "Epoch Step: 155 Acc: 0.46875\n",
      "Epoch Step: 156 Acc: 0.4453125\n",
      "Epoch Step: 157 Acc: 0.421875\n",
      "Epoch Step: 158 Acc: 0.484375\n",
      "Epoch Step: 159 Acc: 0.4453125\n",
      "Epoch Step: 160 Acc: 0.4453125\n",
      "Epoch Step: 161 Acc: 0.5546875\n",
      "Epoch Step: 162 Acc: 0.5390625\n",
      "Epoch Step: 163 Acc: 0.5078125\n",
      "Epoch Step: 164 Acc: 0.5234375\n",
      "Epoch Step: 165 Acc: 0.3984375\n",
      "Epoch Step: 166 Acc: 0.4453125\n",
      "Epoch Step: 167 Acc: 0.4140625\n",
      "Epoch Step: 168 Acc: 0.5390625\n",
      "Epoch Step: 169 Acc: 0.4375\n",
      "Epoch Step: 170 Acc: 0.4765625\n",
      "Epoch Step: 171 Acc: 0.4296875\n",
      "Epoch Step: 172 Acc: 0.5\n",
      "Epoch Step: 173 Acc: 0.4609375\n",
      "Epoch Step: 174 Acc: 0.453125\n",
      "Epoch Step: 175 Acc: 0.46875\n",
      "Epoch Step: 176 Acc: 0.4921875\n",
      "Epoch Step: 177 Acc: 0.4140625\n",
      "Epoch Step: 178 Acc: 0.59375\n",
      "Epoch Step: 179 Acc: 0.5546875\n",
      "Epoch Step: 180 Acc: 0.5\n",
      "Epoch Step: 181 Acc: 0.3984375\n",
      "Epoch Step: 182 Acc: 0.5078125\n",
      "Epoch Step: 183 Acc: 0.546875\n",
      "Epoch Step: 184 Acc: 0.609375\n",
      "Epoch Step: 185 Acc: 0.5078125\n",
      "Epoch Step: 186 Acc: 0.546875\n",
      "Epoch Step: 187 Acc: 0.53125\n",
      "Epoch Step: 188 Acc: 0.421875\n",
      "Epoch Step: 189 Acc: 0.421875\n",
      "Epoch Step: 190 Acc: 0.4453125\n",
      "Epoch Step: 191 Acc: 0.5\n",
      "Epoch Step: 192 Acc: 0.4375\n",
      "Epoch Step: 193 Acc: 0.53125\n",
      "Epoch Step: 194 Acc: 0.5625\n",
      "Epoch Step: 195 Acc: 0.5390625\n",
      "Epoch Step: 196 Acc: 0.46875\n",
      "Epoch Step: 197 Acc: 0.515625\n",
      "Epoch Step: 198 Acc: 0.5\n",
      "Epoch Step: 199 Acc: 0.5078125\n",
      "Epoch Step: 200 Acc: 0.4140625\n",
      "Epoch Step: 201 Acc: 0.578125\n",
      "Epoch Step: 202 Acc: 0.53125\n",
      "Epoch Step: 203 Acc: 0.546875\n",
      "Epoch Step: 204 Acc: 0.421875\n",
      "Epoch Step: 205 Acc: 0.4921875\n",
      "Epoch Step: 206 Acc: 0.4765625\n",
      "Epoch Step: 207 Acc: 0.4921875\n",
      "Epoch Step: 208 Acc: 0.515625\n",
      "Epoch Step: 209 Acc: 0.5390625\n",
      "Epoch Step: 210 Acc: 0.4609375\n",
      "Epoch Step: 211 Acc: 0.5390625\n",
      "Epoch Step: 212 Acc: 0.5078125\n",
      "Epoch Step: 213 Acc: 0.46875\n",
      "Epoch Step: 214 Acc: 0.5234375\n",
      "Epoch Step: 215 Acc: 0.4375\n",
      "Epoch Step: 216 Acc: 0.46875\n",
      "Epoch Step: 217 Acc: 0.4375\n",
      "Epoch Step: 218 Acc: 0.5\n",
      "Epoch Step: 219 Acc: 0.515625\n",
      "Epoch Step: 220 Acc: 0.5\n",
      "Epoch Step: 221 Acc: 0.546875\n",
      "Epoch Step: 222 Acc: 0.3828125\n",
      "Epoch Step: 223 Acc: 0.515625\n",
      "Epoch Step: 224 Acc: 0.46875\n",
      "Epoch Step: 225 Acc: 0.4765625\n",
      "Epoch Step: 226 Acc: 0.5390625\n",
      "Epoch Step: 227 Acc: 0.5390625\n",
      "Epoch Step: 228 Acc: 0.4609375\n",
      "Epoch Step: 229 Acc: 0.53125\n",
      "Epoch Step: 230 Acc: 0.53125\n",
      "Epoch Step: 231 Acc: 0.515625\n",
      "Epoch Step: 232 Acc: 0.5\n",
      "Epoch Step: 233 Acc: 0.53125\n",
      "Epoch Step: 234 Acc: 0.5078125\n",
      "Epoch Step: 235 Acc: 0.5078125\n",
      "Epoch Step: 236 Acc: 0.515625\n",
      "Epoch Step: 237 Acc: 0.4921875\n",
      "Epoch Step: 238 Acc: 0.4921875\n",
      "Epoch Step: 239 Acc: 0.4765625\n",
      "Epoch Step: 240 Acc: 0.5546875\n",
      "Epoch Step: 241 Acc: 0.453125\n",
      "Epoch Step: 242 Acc: 0.4296875\n",
      "Epoch Step: 243 Acc: 0.53125\n",
      "Epoch Step: 244 Acc: 0.4921875\n",
      "Epoch Step: 245 Acc: 0.46875\n",
      "Epoch Step: 246 Acc: 0.4765625\n",
      "Epoch Step: 247 Acc: 0.5234375\n",
      "Epoch Step: 248 Acc: 0.484375\n",
      "Epoch Step: 249 Acc: 0.5234375\n",
      "Epoch Step: 250 Acc: 0.5078125\n",
      "Epoch Step: 251 Acc: 0.5\n",
      "Epoch Step: 252 Acc: 0.4453125\n",
      "Epoch Step: 253 Acc: 0.4140625\n",
      "Epoch Step: 254 Acc: 0.5078125\n",
      "Epoch Step: 255 Acc: 0.40625\n",
      "Epoch Step: 256 Acc: 0.4765625\n",
      "Epoch Step: 257 Acc: 0.5078125\n",
      "Epoch Step: 258 Acc: 0.5390625\n",
      "Epoch Step: 259 Acc: 0.4765625\n",
      "Epoch Step: 260 Acc: 0.4765625\n",
      "Epoch Step: 261 Acc: 0.4765625\n",
      "Epoch Step: 262 Acc: 0.609375\n",
      "Epoch Step: 263 Acc: 0.5390625\n",
      "Epoch Step: 264 Acc: 0.46875\n",
      "Epoch Step: 265 Acc: 0.5859375\n",
      "Epoch Step: 266 Acc: 0.5234375\n",
      "Epoch Step: 267 Acc: 0.4921875\n",
      "Epoch Step: 268 Acc: 0.5\n",
      "Epoch Step: 269 Acc: 0.484375\n",
      "Epoch Step: 270 Acc: 0.4921875\n",
      "Epoch Step: 271 Acc: 0.4765625\n",
      "Epoch Step: 272 Acc: 0.53125\n",
      "Epoch Step: 273 Acc: 0.4453125\n",
      "Epoch Step: 274 Acc: 0.40625\n",
      "Epoch Step: 275 Acc: 0.375\n",
      "Epoch Step: 276 Acc: 0.4921875\n",
      "Epoch Step: 277 Acc: 0.4921875\n",
      "Epoch Step: 278 Acc: 0.5234375\n",
      "Epoch Step: 279 Acc: 0.5\n",
      "Epoch Step: 280 Acc: 0.3671875\n",
      "Epoch Step: 281 Acc: 0.46875\n",
      "Epoch Step: 282 Acc: 0.5234375\n",
      "Epoch Step: 283 Acc: 0.5078125\n",
      "Epoch Step: 284 Acc: 0.5078125\n",
      "Epoch Step: 285 Acc: 0.46875\n",
      "Epoch Step: 286 Acc: 0.46875\n",
      "Epoch Step: 287 Acc: 0.453125\n",
      "Epoch Step: 288 Acc: 0.484375\n",
      "Epoch Step: 289 Acc: 0.4609375\n",
      "Epoch Step: 290 Acc: 0.4453125\n",
      "Epoch Step: 291 Acc: 0.5390625\n",
      "Epoch Step: 292 Acc: 0.546875\n",
      "Epoch Step: 293 Acc: 0.390625\n",
      "Epoch Step: 294 Acc: 0.4375\n",
      "Epoch Step: 295 Acc: 0.5390625\n",
      "Epoch Step: 296 Acc: 0.4609375\n",
      "Epoch Step: 297 Acc: 0.3984375\n",
      "Epoch Step: 298 Acc: 0.46875\n",
      "Epoch Step: 299 Acc: 0.5234375\n",
      "Epoch Step: 300 Acc: 0.4609375\n",
      "Epoch Step: 301 Acc: 0.546875\n",
      "Epoch Step: 302 Acc: 0.4296875\n",
      "Epoch Step: 303 Acc: 0.5703125\n",
      "Epoch Step: 304 Acc: 0.4921875\n",
      "Epoch Step: 305 Acc: 0.5078125\n",
      "Epoch Step: 306 Acc: 0.5390625\n",
      "Epoch Step: 307 Acc: 0.578125\n",
      "Epoch Step: 308 Acc: 0.46875\n",
      "Epoch Step: 309 Acc: 0.546875\n",
      "Epoch Step: 310 Acc: 0.4609375\n",
      "Epoch Step: 311 Acc: 0.4609375\n",
      "Epoch Step: 312 Acc: 0.5234375\n",
      "Epoch Step: 313 Acc: 0.5234375\n",
      "Epoch Step: 314 Acc: 0.5546875\n",
      "Epoch Step: 315 Acc: 0.484375\n",
      "Epoch Step: 316 Acc: 0.5703125\n",
      "Epoch Step: 317 Acc: 0.484375\n",
      "Epoch Step: 318 Acc: 0.4296875\n",
      "Epoch Step: 319 Acc: 0.4921875\n",
      "Epoch Step: 320 Acc: 0.4765625\n",
      "Epoch Step: 321 Acc: 0.46875\n",
      "Epoch Step: 322 Acc: 0.421875\n",
      "Epoch Step: 323 Acc: 0.453125\n",
      "Epoch Step: 324 Acc: 0.4609375\n",
      "Epoch Step: 325 Acc: 0.515625\n",
      "Epoch Step: 326 Acc: 0.453125\n",
      "Epoch Step: 327 Acc: 0.5625\n",
      "Epoch Step: 328 Acc: 0.453125\n",
      "Epoch Step: 329 Acc: 0.4765625\n",
      "Epoch Step: 330 Acc: 0.515625\n",
      "Epoch Step: 331 Acc: 0.4375\n",
      "Epoch Step: 332 Acc: 0.421875\n",
      "Epoch Step: 333 Acc: 0.46875\n",
      "Epoch Step: 334 Acc: 0.484375\n",
      "Epoch Step: 335 Acc: 0.4140625\n",
      "Epoch Step: 336 Acc: 0.484375\n",
      "Epoch Step: 337 Acc: 0.4765625\n",
      "Epoch Step: 338 Acc: 0.46875\n",
      "Epoch Step: 339 Acc: 0.609375\n",
      "Epoch Step: 340 Acc: 0.4375\n",
      "Epoch Step: 341 Acc: 0.421875\n",
      "Epoch Step: 342 Acc: 0.5546875\n",
      "Epoch Step: 343 Acc: 0.4375\n",
      "Epoch Step: 344 Acc: 0.5078125\n",
      "Epoch Step: 345 Acc: 0.4296875\n",
      "Epoch Step: 346 Acc: 0.5234375\n",
      "Epoch Step: 347 Acc: 0.4375\n",
      "Epoch Step: 348 Acc: 0.4921875\n",
      "Epoch Step: 349 Acc: 0.46875\n",
      "Epoch Step: 350 Acc: 0.4296875\n",
      "Epoch Step: 351 Acc: 0.515625\n",
      "Epoch Step: 352 Acc: 0.5625\n",
      "Epoch Step: 353 Acc: 0.40625\n",
      "Epoch Step: 354 Acc: 0.5\n",
      "Epoch Step: 355 Acc: 0.53125\n",
      "Epoch Step: 356 Acc: 0.4296875\n",
      "Epoch Step: 357 Acc: 0.5234375\n",
      "Epoch Step: 358 Acc: 0.4375\n",
      "Epoch Step: 359 Acc: 0.4296875\n",
      "Epoch Step: 360 Acc: 0.5078125\n",
      "Epoch Step: 361 Acc: 0.5859375\n",
      "Epoch Step: 362 Acc: 0.5078125\n",
      "Epoch Step: 363 Acc: 0.5625\n",
      "Epoch Step: 364 Acc: 0.4296875\n",
      "Epoch Step: 365 Acc: 0.46875\n",
      "Epoch Step: 366 Acc: 0.5234375\n",
      "Epoch Step: 367 Acc: 0.5078125\n",
      "Epoch Step: 368 Acc: 0.609375\n",
      "Epoch Step: 369 Acc: 0.4609375\n",
      "Epoch Step: 370 Acc: 0.3984375\n",
      "Epoch Step: 371 Acc: 0.484375\n",
      "Epoch Step: 372 Acc: 0.46875\n",
      "Epoch Step: 373 Acc: 0.484375\n",
      "Epoch Step: 374 Acc: 0.4453125\n",
      "Epoch Step: 375 Acc: 0.421875\n",
      "Epoch Step: 376 Acc: 0.5390625\n",
      "Epoch Step: 377 Acc: 0.5078125\n",
      "Epoch Step: 378 Acc: 0.4765625\n",
      "Epoch Step: 379 Acc: 0.4921875\n",
      "Epoch Step: 380 Acc: 0.53125\n",
      "Epoch Step: 381 Acc: 0.5234375\n",
      "Epoch Step: 382 Acc: 0.5234375\n",
      "Epoch Step: 383 Acc: 0.4140625\n",
      "Epoch Step: 384 Acc: 0.5078125\n",
      "Epoch Step: 385 Acc: 0.515625\n",
      "Epoch Step: 386 Acc: 0.4375\n",
      "Epoch Step: 387 Acc: 0.3828125\n",
      "Epoch Step: 388 Acc: 0.4765625\n",
      "Epoch Step: 389 Acc: 0.4609375\n",
      "Epoch Step: 390 Acc: 0.515625\n",
      "Epoch Step: 391 Acc: 0.4296875\n",
      "Epoch Step: 392 Acc: 0.5546875\n",
      "Epoch Step: 393 Acc: 0.53125\n",
      "Epoch Step: 394 Acc: 0.484375\n",
      "Epoch Step: 395 Acc: 0.4765625\n",
      "Epoch Step: 396 Acc: 0.5234375\n",
      "Epoch Step: 397 Acc: 0.53125\n",
      "Epoch Step: 398 Acc: 0.515625\n",
      "Epoch Step: 399 Acc: 0.4296875\n",
      "Epoch Step: 400 Acc: 0.546875\n",
      "Epoch Step: 401 Acc: 0.515625\n",
      "Epoch Step: 402 Acc: 0.4375\n",
      "Epoch Step: 403 Acc: 0.4453125\n",
      "Epoch Step: 404 Acc: 0.453125\n",
      "Epoch Step: 405 Acc: 0.4765625\n",
      "Epoch Step: 406 Acc: 0.484375\n",
      "Epoch Step: 407 Acc: 0.5234375\n",
      "Epoch Step: 408 Acc: 0.5546875\n",
      "Epoch Step: 409 Acc: 0.515625\n",
      "Epoch Step: 410 Acc: 0.4921875\n",
      "Epoch Step: 411 Acc: 0.4375\n",
      "Epoch Step: 412 Acc: 0.4609375\n",
      "Epoch Step: 413 Acc: 0.5078125\n",
      "Epoch Step: 414 Acc: 0.4765625\n",
      "Epoch Step: 415 Acc: 0.5390625\n",
      "Epoch Step: 416 Acc: 0.4921875\n",
      "Epoch Step: 417 Acc: 0.5078125\n",
      "Epoch Step: 418 Acc: 0.4921875\n",
      "Epoch Step: 419 Acc: 0.453125\n",
      "Epoch Step: 420 Acc: 0.5546875\n",
      "Epoch Step: 421 Acc: 0.5\n",
      "Epoch Step: 422 Acc: 0.5546875\n",
      "Epoch Step: 423 Acc: 0.4609375\n",
      "Epoch Step: 424 Acc: 0.4375\n",
      "Epoch Step: 425 Acc: 0.5\n",
      "Epoch Step: 426 Acc: 0.515625\n",
      "Epoch Step: 427 Acc: 0.546875\n",
      "Epoch Step: 428 Acc: 0.4375\n",
      "Epoch Step: 429 Acc: 0.46875\n",
      "Epoch Step: 430 Acc: 0.5\n",
      "Epoch Step: 431 Acc: 0.4609375\n",
      "Epoch Step: 432 Acc: 0.4765625\n",
      "Epoch Step: 433 Acc: 0.515625\n",
      "Epoch Step: 434 Acc: 0.46875\n",
      "Epoch Step: 435 Acc: 0.4765625\n",
      "Epoch Step: 436 Acc: 0.515625\n",
      "Epoch Step: 437 Acc: 0.46875\n",
      "Epoch Step: 438 Acc: 0.4921875\n",
      "Epoch Step: 439 Acc: 0.484375\n",
      "Epoch Step: 440 Acc: 0.4296875\n",
      "Epoch Step: 441 Acc: 0.53125\n",
      "Epoch Step: 442 Acc: 0.453125\n",
      "Epoch Step: 443 Acc: 0.5078125\n",
      "Epoch Step: 444 Acc: 0.4765625\n",
      "Epoch Step: 445 Acc: 0.5\n",
      "Epoch Step: 446 Acc: 0.46875\n",
      "Epoch Step: 447 Acc: 0.4453125\n",
      "Epoch Step: 448 Acc: 0.53125\n",
      "Epoch Step: 449 Acc: 0.5390625\n",
      "Epoch Step: 450 Acc: 0.4375\n",
      "Epoch Step: 451 Acc: 0.5078125\n",
      "Epoch Step: 452 Acc: 0.5078125\n",
      "Epoch Step: 453 Acc: 0.4296875\n",
      "Epoch Step: 454 Acc: 0.4453125\n",
      "Epoch Step: 455 Acc: 0.5390625\n",
      "Epoch Step: 456 Acc: 0.4296875\n",
      "Epoch Step: 457 Acc: 0.3984375\n",
      "Epoch Step: 458 Acc: 0.5\n",
      "Epoch Step: 459 Acc: 0.390625\n",
      "Epoch Step: 460 Acc: 0.484375\n",
      "Epoch Step: 461 Acc: 0.5078125\n",
      "Epoch Step: 462 Acc: 0.515625\n",
      "Epoch Step: 463 Acc: 0.46875\n",
      "Epoch Step: 464 Acc: 0.46875\n",
      "Epoch Step: 465 Acc: 0.4453125\n",
      "Epoch Step: 466 Acc: 0.5\n",
      "Epoch Step: 467 Acc: 0.546875\n",
      "Epoch Step: 468 Acc: 0.4609375\n",
      "Epoch Step: 469 Acc: 0.5078125\n",
      "Epoch Step: 470 Acc: 0.53125\n",
      "Epoch Step: 471 Acc: 0.484375\n",
      "Epoch Step: 472 Acc: 0.421875\n",
      "Epoch Step: 473 Acc: 0.484375\n",
      "Epoch Step: 474 Acc: 0.4453125\n",
      "Epoch Step: 475 Acc: 0.4296875\n",
      "Epoch Step: 476 Acc: 0.546875\n",
      "Epoch Step: 477 Acc: 0.5\n",
      "Epoch Step: 478 Acc: 0.5\n",
      "Epoch Step: 479 Acc: 0.515625\n",
      "Epoch Step: 480 Acc: 0.4765625\n",
      "Epoch Step: 481 Acc: 0.46875\n",
      "Epoch Step: 482 Acc: 0.46875\n",
      "Epoch Step: 483 Acc: 0.5\n",
      "Epoch Step: 484 Acc: 0.421875\n",
      "Epoch Step: 485 Acc: 0.453125\n",
      "Epoch Step: 486 Acc: 0.5234375\n",
      "Epoch Step: 487 Acc: 0.515625\n",
      "Epoch Step: 488 Acc: 0.5234375\n",
      "Epoch Step: 489 Acc: 0.421875\n",
      "Epoch Step: 490 Acc: 0.4453125\n",
      "Epoch Step: 491 Acc: 0.5390625\n",
      "Epoch Step: 492 Acc: 0.4921875\n",
      "Epoch Step: 493 Acc: 0.484375\n",
      "Epoch Step: 494 Acc: 0.453125\n",
      "Epoch Step: 495 Acc: 0.4765625\n",
      "Epoch Step: 496 Acc: 0.46875\n",
      "Epoch Step: 497 Acc: 0.5703125\n",
      "Epoch Step: 498 Acc: 0.4765625\n",
      "Epoch Step: 499 Acc: 0.4375\n",
      "Epoch Step: 500 Acc: 0.4609375\n",
      "Epoch Step: 501 Acc: 0.453125\n",
      "Epoch Step: 502 Acc: 0.4296875\n",
      "Epoch Step: 503 Acc: 0.5078125\n",
      "Epoch Step: 504 Acc: 0.515625\n",
      "Epoch Step: 505 Acc: 0.5390625\n",
      "Epoch Step: 506 Acc: 0.4140625\n",
      "Epoch Step: 507 Acc: 0.4609375\n",
      "Epoch Step: 508 Acc: 0.391304347826087\n"
     ]
    }
   ],
   "source": [
    "for batch, (src_ids, src_len, tgt) in enumerate(test_dataloader):\n",
    "    src_ids = src_ids.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    logits = trained_model(src_ids, src_len)\n",
    "    print(f\"Epoch Step: {batch} Acc: {(logits.argmax(1) == tgt).sum().item() / tgt.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 0 Acc: 0.4609375\n",
      "Epoch Step: 1 Acc: 0.453125\n",
      "Epoch Step: 2 Acc: 0.4609375\n",
      "Epoch Step: 3 Acc: 0.484375\n",
      "Epoch Step: 4 Acc: 0.5703125\n",
      "Epoch Step: 5 Acc: 0.46875\n",
      "Epoch Step: 6 Acc: 0.5078125\n",
      "Epoch Step: 7 Acc: 0.46875\n",
      "Epoch Step: 8 Acc: 0.453125\n",
      "Epoch Step: 9 Acc: 0.390625\n",
      "Epoch Step: 10 Acc: 0.4296875\n",
      "Epoch Step: 11 Acc: 0.484375\n",
      "Epoch Step: 12 Acc: 0.453125\n",
      "Epoch Step: 13 Acc: 0.5\n",
      "Epoch Step: 14 Acc: 0.46875\n",
      "Epoch Step: 15 Acc: 0.5625\n",
      "Epoch Step: 16 Acc: 0.46875\n",
      "Epoch Step: 17 Acc: 0.46875\n",
      "Epoch Step: 18 Acc: 0.5\n",
      "Epoch Step: 19 Acc: 0.484375\n",
      "Epoch Step: 20 Acc: 0.4609375\n",
      "Epoch Step: 21 Acc: 0.5703125\n",
      "Epoch Step: 22 Acc: 0.453125\n",
      "Epoch Step: 23 Acc: 0.4453125\n",
      "Epoch Step: 24 Acc: 0.4921875\n",
      "Epoch Step: 25 Acc: 0.5078125\n",
      "Epoch Step: 26 Acc: 0.4140625\n",
      "Epoch Step: 27 Acc: 0.4609375\n",
      "Epoch Step: 28 Acc: 0.5078125\n",
      "Epoch Step: 29 Acc: 0.5390625\n",
      "Epoch Step: 30 Acc: 0.453125\n",
      "Epoch Step: 31 Acc: 0.46875\n",
      "Epoch Step: 32 Acc: 0.5078125\n",
      "Epoch Step: 33 Acc: 0.5390625\n",
      "Epoch Step: 34 Acc: 0.4375\n",
      "Epoch Step: 35 Acc: 0.4296875\n",
      "Epoch Step: 36 Acc: 0.4921875\n",
      "Epoch Step: 37 Acc: 0.546875\n",
      "Epoch Step: 38 Acc: 0.5\n",
      "Epoch Step: 39 Acc: 0.46875\n",
      "Epoch Step: 40 Acc: 0.5078125\n",
      "Epoch Step: 41 Acc: 0.5078125\n",
      "Epoch Step: 42 Acc: 0.421875\n",
      "Epoch Step: 43 Acc: 0.46875\n",
      "Epoch Step: 44 Acc: 0.5390625\n",
      "Epoch Step: 45 Acc: 0.4296875\n",
      "Epoch Step: 46 Acc: 0.515625\n",
      "Epoch Step: 47 Acc: 0.4921875\n",
      "Epoch Step: 48 Acc: 0.46875\n",
      "Epoch Step: 49 Acc: 0.453125\n",
      "Epoch Step: 50 Acc: 0.4609375\n",
      "Epoch Step: 51 Acc: 0.4921875\n",
      "Epoch Step: 52 Acc: 0.515625\n",
      "Epoch Step: 53 Acc: 0.5\n",
      "Epoch Step: 54 Acc: 0.484375\n",
      "Epoch Step: 55 Acc: 0.5078125\n",
      "Epoch Step: 56 Acc: 0.453125\n",
      "Epoch Step: 57 Acc: 0.578125\n",
      "Epoch Step: 58 Acc: 0.3984375\n",
      "Epoch Step: 59 Acc: 0.4296875\n",
      "Epoch Step: 60 Acc: 0.5390625\n",
      "Epoch Step: 61 Acc: 0.4765625\n",
      "Epoch Step: 62 Acc: 0.375\n",
      "Epoch Step: 63 Acc: 0.46875\n",
      "Epoch Step: 64 Acc: 0.4765625\n",
      "Epoch Step: 65 Acc: 0.4296875\n",
      "Epoch Step: 66 Acc: 0.5\n",
      "Epoch Step: 67 Acc: 0.4375\n",
      "Epoch Step: 68 Acc: 0.4140625\n",
      "Epoch Step: 69 Acc: 0.4375\n",
      "Epoch Step: 70 Acc: 0.515625\n",
      "Epoch Step: 71 Acc: 0.46875\n",
      "Epoch Step: 72 Acc: 0.5625\n",
      "Epoch Step: 73 Acc: 0.484375\n",
      "Epoch Step: 74 Acc: 0.4609375\n",
      "Epoch Step: 75 Acc: 0.453125\n",
      "Epoch Step: 76 Acc: 0.4375\n",
      "Epoch Step: 77 Acc: 0.5546875\n",
      "Epoch Step: 78 Acc: 0.4609375\n",
      "Epoch Step: 79 Acc: 0.453125\n",
      "Epoch Step: 80 Acc: 0.453125\n",
      "Epoch Step: 81 Acc: 0.453125\n",
      "Epoch Step: 82 Acc: 0.5546875\n",
      "Epoch Step: 83 Acc: 0.46875\n",
      "Epoch Step: 84 Acc: 0.4296875\n",
      "Epoch Step: 85 Acc: 0.421875\n",
      "Epoch Step: 86 Acc: 0.4765625\n",
      "Epoch Step: 87 Acc: 0.4296875\n",
      "Epoch Step: 88 Acc: 0.5234375\n",
      "Epoch Step: 89 Acc: 0.5078125\n",
      "Epoch Step: 90 Acc: 0.53125\n",
      "Epoch Step: 91 Acc: 0.484375\n",
      "Epoch Step: 92 Acc: 0.4453125\n",
      "Epoch Step: 93 Acc: 0.4765625\n",
      "Epoch Step: 94 Acc: 0.4609375\n",
      "Epoch Step: 95 Acc: 0.6171875\n",
      "Epoch Step: 96 Acc: 0.5\n",
      "Epoch Step: 97 Acc: 0.453125\n",
      "Epoch Step: 98 Acc: 0.5234375\n",
      "Epoch Step: 99 Acc: 0.4296875\n",
      "Epoch Step: 100 Acc: 0.4921875\n",
      "Epoch Step: 101 Acc: 0.4296875\n",
      "Epoch Step: 102 Acc: 0.5546875\n",
      "Epoch Step: 103 Acc: 0.46875\n",
      "Epoch Step: 104 Acc: 0.4453125\n",
      "Epoch Step: 105 Acc: 0.4296875\n",
      "Epoch Step: 106 Acc: 0.5\n",
      "Epoch Step: 107 Acc: 0.375\n",
      "Epoch Step: 108 Acc: 0.5390625\n",
      "Epoch Step: 109 Acc: 0.4140625\n",
      "Epoch Step: 110 Acc: 0.53125\n",
      "Epoch Step: 111 Acc: 0.46875\n",
      "Epoch Step: 112 Acc: 0.4921875\n",
      "Epoch Step: 113 Acc: 0.546875\n",
      "Epoch Step: 114 Acc: 0.3828125\n",
      "Epoch Step: 115 Acc: 0.4296875\n",
      "Epoch Step: 116 Acc: 0.4609375\n",
      "Epoch Step: 117 Acc: 0.578125\n",
      "Epoch Step: 118 Acc: 0.484375\n",
      "Epoch Step: 119 Acc: 0.4296875\n",
      "Epoch Step: 120 Acc: 0.484375\n",
      "Epoch Step: 121 Acc: 0.484375\n",
      "Epoch Step: 122 Acc: 0.421875\n",
      "Epoch Step: 123 Acc: 0.546875\n",
      "Epoch Step: 124 Acc: 0.4296875\n",
      "Epoch Step: 125 Acc: 0.4296875\n",
      "Epoch Step: 126 Acc: 0.453125\n",
      "Epoch Step: 127 Acc: 0.3984375\n",
      "Epoch Step: 128 Acc: 0.421875\n",
      "Epoch Step: 129 Acc: 0.4375\n",
      "Epoch Step: 130 Acc: 0.453125\n",
      "Epoch Step: 131 Acc: 0.4296875\n",
      "Epoch Step: 132 Acc: 0.5625\n",
      "Epoch Step: 133 Acc: 0.515625\n",
      "Epoch Step: 134 Acc: 0.46875\n",
      "Epoch Step: 135 Acc: 0.46875\n",
      "Epoch Step: 136 Acc: 0.4375\n",
      "Epoch Step: 137 Acc: 0.5\n",
      "Epoch Step: 138 Acc: 0.421875\n",
      "Epoch Step: 139 Acc: 0.453125\n",
      "Epoch Step: 140 Acc: 0.453125\n",
      "Epoch Step: 141 Acc: 0.484375\n",
      "Epoch Step: 142 Acc: 0.4921875\n",
      "Epoch Step: 143 Acc: 0.4453125\n",
      "Epoch Step: 144 Acc: 0.5234375\n",
      "Epoch Step: 145 Acc: 0.5078125\n",
      "Epoch Step: 146 Acc: 0.5234375\n",
      "Epoch Step: 147 Acc: 0.5546875\n",
      "Epoch Step: 148 Acc: 0.4453125\n",
      "Epoch Step: 149 Acc: 0.5859375\n",
      "Epoch Step: 150 Acc: 0.5\n",
      "Epoch Step: 151 Acc: 0.4921875\n",
      "Epoch Step: 152 Acc: 0.5390625\n",
      "Epoch Step: 153 Acc: 0.484375\n",
      "Epoch Step: 154 Acc: 0.4765625\n",
      "Epoch Step: 155 Acc: 0.4765625\n",
      "Epoch Step: 156 Acc: 0.453125\n",
      "Epoch Step: 157 Acc: 0.4921875\n",
      "Epoch Step: 158 Acc: 0.59375\n",
      "Epoch Step: 159 Acc: 0.4296875\n",
      "Epoch Step: 160 Acc: 0.4921875\n",
      "Epoch Step: 161 Acc: 0.4453125\n",
      "Epoch Step: 162 Acc: 0.4453125\n",
      "Epoch Step: 163 Acc: 0.4609375\n",
      "Epoch Step: 164 Acc: 0.4609375\n",
      "Epoch Step: 165 Acc: 0.46875\n",
      "Epoch Step: 166 Acc: 0.5\n",
      "Epoch Step: 167 Acc: 0.453125\n",
      "Epoch Step: 168 Acc: 0.4609375\n",
      "Epoch Step: 169 Acc: 0.4453125\n",
      "Epoch Step: 170 Acc: 0.4140625\n",
      "Epoch Step: 171 Acc: 0.5390625\n",
      "Epoch Step: 172 Acc: 0.4453125\n",
      "Epoch Step: 173 Acc: 0.4453125\n",
      "Epoch Step: 174 Acc: 0.4453125\n",
      "Epoch Step: 175 Acc: 0.515625\n",
      "Epoch Step: 176 Acc: 0.4453125\n",
      "Epoch Step: 177 Acc: 0.5078125\n",
      "Epoch Step: 178 Acc: 0.484375\n",
      "Epoch Step: 179 Acc: 0.4375\n",
      "Epoch Step: 180 Acc: 0.4375\n",
      "Epoch Step: 181 Acc: 0.5078125\n",
      "Epoch Step: 182 Acc: 0.4609375\n",
      "Epoch Step: 183 Acc: 0.4453125\n",
      "Epoch Step: 184 Acc: 0.484375\n",
      "Epoch Step: 185 Acc: 0.46875\n",
      "Epoch Step: 186 Acc: 0.4453125\n",
      "Epoch Step: 187 Acc: 0.4140625\n",
      "Epoch Step: 188 Acc: 0.5625\n",
      "Epoch Step: 189 Acc: 0.4296875\n",
      "Epoch Step: 190 Acc: 0.484375\n",
      "Epoch Step: 191 Acc: 0.453125\n",
      "Epoch Step: 192 Acc: 0.4140625\n",
      "Epoch Step: 193 Acc: 0.4765625\n",
      "Epoch Step: 194 Acc: 0.4609375\n",
      "Epoch Step: 195 Acc: 0.5\n",
      "Epoch Step: 196 Acc: 0.40625\n",
      "Epoch Step: 197 Acc: 0.453125\n",
      "Epoch Step: 198 Acc: 0.46875\n",
      "Epoch Step: 199 Acc: 0.4765625\n",
      "Epoch Step: 200 Acc: 0.453125\n",
      "Epoch Step: 201 Acc: 0.4765625\n",
      "Epoch Step: 202 Acc: 0.515625\n",
      "Epoch Step: 203 Acc: 0.421875\n",
      "Epoch Step: 204 Acc: 0.515625\n",
      "Epoch Step: 205 Acc: 0.4453125\n",
      "Epoch Step: 206 Acc: 0.5\n",
      "Epoch Step: 207 Acc: 0.453125\n",
      "Epoch Step: 208 Acc: 0.421875\n",
      "Epoch Step: 209 Acc: 0.5546875\n",
      "Epoch Step: 210 Acc: 0.5\n",
      "Epoch Step: 211 Acc: 0.5625\n",
      "Epoch Step: 212 Acc: 0.4375\n",
      "Epoch Step: 213 Acc: 0.4140625\n",
      "Epoch Step: 214 Acc: 0.453125\n",
      "Epoch Step: 215 Acc: 0.4609375\n",
      "Epoch Step: 216 Acc: 0.40625\n",
      "Epoch Step: 217 Acc: 0.4375\n",
      "Epoch Step: 218 Acc: 0.4765625\n",
      "Epoch Step: 219 Acc: 0.484375\n",
      "Epoch Step: 220 Acc: 0.5546875\n",
      "Epoch Step: 221 Acc: 0.484375\n",
      "Epoch Step: 222 Acc: 0.4609375\n",
      "Epoch Step: 223 Acc: 0.5078125\n",
      "Epoch Step: 224 Acc: 0.46875\n",
      "Epoch Step: 225 Acc: 0.546875\n",
      "Epoch Step: 226 Acc: 0.4921875\n",
      "Epoch Step: 227 Acc: 0.515625\n",
      "Epoch Step: 228 Acc: 0.5078125\n",
      "Epoch Step: 229 Acc: 0.4609375\n",
      "Epoch Step: 230 Acc: 0.46875\n",
      "Epoch Step: 231 Acc: 0.3984375\n",
      "Epoch Step: 232 Acc: 0.4921875\n",
      "Epoch Step: 233 Acc: 0.484375\n",
      "Epoch Step: 234 Acc: 0.3984375\n",
      "Epoch Step: 235 Acc: 0.4140625\n",
      "Epoch Step: 236 Acc: 0.4453125\n",
      "Epoch Step: 237 Acc: 0.4140625\n",
      "Epoch Step: 238 Acc: 0.5078125\n",
      "Epoch Step: 239 Acc: 0.453125\n",
      "Epoch Step: 240 Acc: 0.4921875\n",
      "Epoch Step: 241 Acc: 0.46875\n",
      "Epoch Step: 242 Acc: 0.46875\n",
      "Epoch Step: 243 Acc: 0.4609375\n",
      "Epoch Step: 244 Acc: 0.5625\n",
      "Epoch Step: 245 Acc: 0.390625\n",
      "Epoch Step: 246 Acc: 0.484375\n",
      "Epoch Step: 247 Acc: 0.46875\n",
      "Epoch Step: 248 Acc: 0.4375\n",
      "Epoch Step: 249 Acc: 0.4765625\n",
      "Epoch Step: 250 Acc: 0.40625\n",
      "Epoch Step: 251 Acc: 0.4453125\n",
      "Epoch Step: 252 Acc: 0.46875\n",
      "Epoch Step: 253 Acc: 0.4140625\n",
      "Epoch Step: 254 Acc: 0.40625\n",
      "Epoch Step: 255 Acc: 0.4609375\n",
      "Epoch Step: 256 Acc: 0.515625\n",
      "Epoch Step: 257 Acc: 0.46875\n",
      "Epoch Step: 258 Acc: 0.59375\n",
      "Epoch Step: 259 Acc: 0.4609375\n",
      "Epoch Step: 260 Acc: 0.40625\n",
      "Epoch Step: 261 Acc: 0.453125\n",
      "Epoch Step: 262 Acc: 0.53125\n",
      "Epoch Step: 263 Acc: 0.484375\n",
      "Epoch Step: 264 Acc: 0.4453125\n",
      "Epoch Step: 265 Acc: 0.546875\n",
      "Epoch Step: 266 Acc: 0.4609375\n",
      "Epoch Step: 267 Acc: 0.484375\n",
      "Epoch Step: 268 Acc: 0.4921875\n",
      "Epoch Step: 269 Acc: 0.5\n",
      "Epoch Step: 270 Acc: 0.4375\n",
      "Epoch Step: 271 Acc: 0.484375\n",
      "Epoch Step: 272 Acc: 0.421875\n",
      "Epoch Step: 273 Acc: 0.5078125\n",
      "Epoch Step: 274 Acc: 0.53125\n",
      "Epoch Step: 275 Acc: 0.4296875\n",
      "Epoch Step: 276 Acc: 0.5546875\n",
      "Epoch Step: 277 Acc: 0.46875\n",
      "Epoch Step: 278 Acc: 0.4921875\n",
      "Epoch Step: 279 Acc: 0.4296875\n",
      "Epoch Step: 280 Acc: 0.5\n",
      "Epoch Step: 281 Acc: 0.5234375\n",
      "Epoch Step: 282 Acc: 0.5\n",
      "Epoch Step: 283 Acc: 0.46875\n",
      "Epoch Step: 284 Acc: 0.421875\n",
      "Epoch Step: 285 Acc: 0.4296875\n",
      "Epoch Step: 286 Acc: 0.453125\n",
      "Epoch Step: 287 Acc: 0.484375\n",
      "Epoch Step: 288 Acc: 0.5\n",
      "Epoch Step: 289 Acc: 0.453125\n",
      "Epoch Step: 290 Acc: 0.453125\n",
      "Epoch Step: 291 Acc: 0.4375\n",
      "Epoch Step: 292 Acc: 0.4921875\n",
      "Epoch Step: 293 Acc: 0.4296875\n",
      "Epoch Step: 294 Acc: 0.4609375\n",
      "Epoch Step: 295 Acc: 0.4375\n",
      "Epoch Step: 296 Acc: 0.484375\n",
      "Epoch Step: 297 Acc: 0.5\n",
      "Epoch Step: 298 Acc: 0.46875\n",
      "Epoch Step: 299 Acc: 0.4921875\n",
      "Epoch Step: 300 Acc: 0.4296875\n",
      "Epoch Step: 301 Acc: 0.4609375\n",
      "Epoch Step: 302 Acc: 0.4140625\n",
      "Epoch Step: 303 Acc: 0.3828125\n",
      "Epoch Step: 304 Acc: 0.5078125\n",
      "Epoch Step: 305 Acc: 0.4140625\n",
      "Epoch Step: 306 Acc: 0.4453125\n",
      "Epoch Step: 307 Acc: 0.4921875\n",
      "Epoch Step: 308 Acc: 0.4609375\n",
      "Epoch Step: 309 Acc: 0.4765625\n",
      "Epoch Step: 310 Acc: 0.4609375\n",
      "Epoch Step: 311 Acc: 0.5859375\n",
      "Epoch Step: 312 Acc: 0.4609375\n",
      "Epoch Step: 313 Acc: 0.4453125\n",
      "Epoch Step: 314 Acc: 0.53125\n",
      "Epoch Step: 315 Acc: 0.421875\n",
      "Epoch Step: 316 Acc: 0.546875\n",
      "Epoch Step: 317 Acc: 0.421875\n",
      "Epoch Step: 318 Acc: 0.46875\n",
      "Epoch Step: 319 Acc: 0.4921875\n",
      "Epoch Step: 320 Acc: 0.53125\n",
      "Epoch Step: 321 Acc: 0.453125\n",
      "Epoch Step: 322 Acc: 0.46875\n",
      "Epoch Step: 323 Acc: 0.4765625\n",
      "Epoch Step: 324 Acc: 0.4609375\n",
      "Epoch Step: 325 Acc: 0.5\n",
      "Epoch Step: 326 Acc: 0.515625\n",
      "Epoch Step: 327 Acc: 0.5\n",
      "Epoch Step: 328 Acc: 0.484375\n",
      "Epoch Step: 329 Acc: 0.4609375\n",
      "Epoch Step: 330 Acc: 0.515625\n",
      "Epoch Step: 331 Acc: 0.46875\n",
      "Epoch Step: 332 Acc: 0.4453125\n",
      "Epoch Step: 333 Acc: 0.4609375\n",
      "Epoch Step: 334 Acc: 0.546875\n",
      "Epoch Step: 335 Acc: 0.390625\n",
      "Epoch Step: 336 Acc: 0.4375\n",
      "Epoch Step: 337 Acc: 0.4921875\n",
      "Epoch Step: 338 Acc: 0.46875\n",
      "Epoch Step: 339 Acc: 0.40625\n",
      "Epoch Step: 340 Acc: 0.4140625\n",
      "Epoch Step: 341 Acc: 0.40625\n",
      "Epoch Step: 342 Acc: 0.5\n",
      "Epoch Step: 343 Acc: 0.5078125\n",
      "Epoch Step: 344 Acc: 0.4921875\n",
      "Epoch Step: 345 Acc: 0.5078125\n",
      "Epoch Step: 346 Acc: 0.4765625\n",
      "Epoch Step: 347 Acc: 0.515625\n",
      "Epoch Step: 348 Acc: 0.4765625\n",
      "Epoch Step: 349 Acc: 0.5078125\n",
      "Epoch Step: 350 Acc: 0.5234375\n",
      "Epoch Step: 351 Acc: 0.5234375\n",
      "Epoch Step: 352 Acc: 0.4765625\n",
      "Epoch Step: 353 Acc: 0.5234375\n",
      "Epoch Step: 354 Acc: 0.4921875\n",
      "Epoch Step: 355 Acc: 0.40625\n",
      "Epoch Step: 356 Acc: 0.484375\n",
      "Epoch Step: 357 Acc: 0.4296875\n",
      "Epoch Step: 358 Acc: 0.5546875\n",
      "Epoch Step: 359 Acc: 0.390625\n",
      "Epoch Step: 360 Acc: 0.5\n",
      "Epoch Step: 361 Acc: 0.34375\n",
      "Epoch Step: 362 Acc: 0.4765625\n",
      "Epoch Step: 363 Acc: 0.4296875\n",
      "Epoch Step: 364 Acc: 0.4453125\n",
      "Epoch Step: 365 Acc: 0.5078125\n",
      "Epoch Step: 366 Acc: 0.515625\n",
      "Epoch Step: 367 Acc: 0.484375\n",
      "Epoch Step: 368 Acc: 0.46875\n",
      "Epoch Step: 369 Acc: 0.390625\n",
      "Epoch Step: 370 Acc: 0.4140625\n",
      "Epoch Step: 371 Acc: 0.4765625\n",
      "Epoch Step: 372 Acc: 0.5625\n",
      "Epoch Step: 373 Acc: 0.4140625\n",
      "Epoch Step: 374 Acc: 0.46875\n",
      "Epoch Step: 375 Acc: 0.4453125\n",
      "Epoch Step: 376 Acc: 0.484375\n",
      "Epoch Step: 377 Acc: 0.4765625\n",
      "Epoch Step: 378 Acc: 0.5078125\n",
      "Epoch Step: 379 Acc: 0.4921875\n",
      "Epoch Step: 380 Acc: 0.5\n",
      "Epoch Step: 381 Acc: 0.5234375\n",
      "Epoch Step: 382 Acc: 0.453125\n",
      "Epoch Step: 383 Acc: 0.5390625\n",
      "Epoch Step: 384 Acc: 0.53125\n",
      "Epoch Step: 385 Acc: 0.5234375\n",
      "Epoch Step: 386 Acc: 0.515625\n",
      "Epoch Step: 387 Acc: 0.5234375\n",
      "Epoch Step: 388 Acc: 0.453125\n",
      "Epoch Step: 389 Acc: 0.5234375\n",
      "Epoch Step: 390 Acc: 0.4609375\n",
      "Epoch Step: 391 Acc: 0.4296875\n",
      "Epoch Step: 392 Acc: 0.3984375\n",
      "Epoch Step: 393 Acc: 0.53125\n",
      "Epoch Step: 394 Acc: 0.453125\n",
      "Epoch Step: 395 Acc: 0.578125\n",
      "Epoch Step: 396 Acc: 0.4453125\n",
      "Epoch Step: 397 Acc: 0.484375\n",
      "Epoch Step: 398 Acc: 0.4609375\n",
      "Epoch Step: 399 Acc: 0.453125\n",
      "Epoch Step: 400 Acc: 0.5234375\n",
      "Epoch Step: 401 Acc: 0.4765625\n",
      "Epoch Step: 402 Acc: 0.484375\n",
      "Epoch Step: 403 Acc: 0.4296875\n",
      "Epoch Step: 404 Acc: 0.4140625\n",
      "Epoch Step: 405 Acc: 0.4296875\n",
      "Epoch Step: 406 Acc: 0.4140625\n",
      "Epoch Step: 407 Acc: 0.5078125\n",
      "Epoch Step: 408 Acc: 0.421875\n",
      "Epoch Step: 409 Acc: 0.4765625\n",
      "Epoch Step: 410 Acc: 0.4140625\n",
      "Epoch Step: 411 Acc: 0.5078125\n",
      "Epoch Step: 412 Acc: 0.453125\n",
      "Epoch Step: 413 Acc: 0.4765625\n",
      "Epoch Step: 414 Acc: 0.484375\n",
      "Epoch Step: 415 Acc: 0.484375\n",
      "Epoch Step: 416 Acc: 0.484375\n",
      "Epoch Step: 417 Acc: 0.4765625\n",
      "Epoch Step: 418 Acc: 0.5078125\n",
      "Epoch Step: 419 Acc: 0.3984375\n",
      "Epoch Step: 420 Acc: 0.5078125\n",
      "Epoch Step: 421 Acc: 0.4453125\n",
      "Epoch Step: 422 Acc: 0.5234375\n",
      "Epoch Step: 423 Acc: 0.5\n",
      "Epoch Step: 424 Acc: 0.5078125\n",
      "Epoch Step: 425 Acc: 0.5\n",
      "Epoch Step: 426 Acc: 0.4609375\n",
      "Epoch Step: 427 Acc: 0.4921875\n",
      "Epoch Step: 428 Acc: 0.5390625\n",
      "Epoch Step: 429 Acc: 0.4375\n",
      "Epoch Step: 430 Acc: 0.453125\n",
      "Epoch Step: 431 Acc: 0.4140625\n",
      "Epoch Step: 432 Acc: 0.453125\n",
      "Epoch Step: 433 Acc: 0.46875\n",
      "Epoch Step: 434 Acc: 0.5078125\n",
      "Epoch Step: 435 Acc: 0.46875\n",
      "Epoch Step: 436 Acc: 0.5078125\n",
      "Epoch Step: 437 Acc: 0.4765625\n",
      "Epoch Step: 438 Acc: 0.53125\n",
      "Epoch Step: 439 Acc: 0.5390625\n",
      "Epoch Step: 440 Acc: 0.453125\n",
      "Epoch Step: 441 Acc: 0.5546875\n",
      "Epoch Step: 442 Acc: 0.4296875\n",
      "Epoch Step: 443 Acc: 0.5\n",
      "Epoch Step: 444 Acc: 0.5\n",
      "Epoch Step: 445 Acc: 0.515625\n",
      "Epoch Step: 446 Acc: 0.4453125\n",
      "Epoch Step: 447 Acc: 0.578125\n",
      "Epoch Step: 448 Acc: 0.53125\n",
      "Epoch Step: 449 Acc: 0.46875\n",
      "Epoch Step: 450 Acc: 0.4296875\n",
      "Epoch Step: 451 Acc: 0.4921875\n",
      "Epoch Step: 452 Acc: 0.4140625\n",
      "Epoch Step: 453 Acc: 0.53125\n",
      "Epoch Step: 454 Acc: 0.6015625\n",
      "Epoch Step: 455 Acc: 0.421875\n",
      "Epoch Step: 456 Acc: 0.421875\n",
      "Epoch Step: 457 Acc: 0.4921875\n",
      "Epoch Step: 458 Acc: 0.5390625\n",
      "Epoch Step: 459 Acc: 0.5\n",
      "Epoch Step: 460 Acc: 0.484375\n",
      "Epoch Step: 461 Acc: 0.40625\n",
      "Epoch Step: 462 Acc: 0.5078125\n",
      "Epoch Step: 463 Acc: 0.5546875\n",
      "Epoch Step: 464 Acc: 0.4296875\n",
      "Epoch Step: 465 Acc: 0.484375\n",
      "Epoch Step: 466 Acc: 0.53125\n",
      "Epoch Step: 467 Acc: 0.5234375\n",
      "Epoch Step: 468 Acc: 0.53125\n",
      "Epoch Step: 469 Acc: 0.5\n",
      "Epoch Step: 470 Acc: 0.484375\n",
      "Epoch Step: 471 Acc: 0.4140625\n",
      "Epoch Step: 472 Acc: 0.5\n",
      "Epoch Step: 473 Acc: 0.46875\n",
      "Epoch Step: 474 Acc: 0.53125\n",
      "Epoch Step: 475 Acc: 0.546875\n",
      "Epoch Step: 476 Acc: 0.453125\n",
      "Epoch Step: 477 Acc: 0.5078125\n",
      "Epoch Step: 478 Acc: 0.5625\n",
      "Epoch Step: 479 Acc: 0.5546875\n",
      "Epoch Step: 480 Acc: 0.4375\n",
      "Epoch Step: 481 Acc: 0.5\n",
      "Epoch Step: 482 Acc: 0.4453125\n",
      "Epoch Step: 483 Acc: 0.4765625\n",
      "Epoch Step: 484 Acc: 0.46875\n",
      "Epoch Step: 485 Acc: 0.5\n",
      "Epoch Step: 486 Acc: 0.46875\n",
      "Epoch Step: 487 Acc: 0.4375\n",
      "Epoch Step: 488 Acc: 0.5\n",
      "Epoch Step: 489 Acc: 0.53125\n",
      "Epoch Step: 490 Acc: 0.4765625\n",
      "Epoch Step: 491 Acc: 0.453125\n",
      "Epoch Step: 492 Acc: 0.4921875\n",
      "Epoch Step: 493 Acc: 0.453125\n",
      "Epoch Step: 494 Acc: 0.4765625\n",
      "Epoch Step: 495 Acc: 0.4765625\n",
      "Epoch Step: 496 Acc: 0.5625\n",
      "Epoch Step: 497 Acc: 0.4453125\n",
      "Epoch Step: 498 Acc: 0.5078125\n",
      "Epoch Step: 499 Acc: 0.53125\n",
      "Epoch Step: 500 Acc: 0.53125\n",
      "Epoch Step: 501 Acc: 0.4375\n",
      "Epoch Step: 502 Acc: 0.4453125\n",
      "Epoch Step: 503 Acc: 0.484375\n",
      "Epoch Step: 504 Acc: 0.4296875\n",
      "Epoch Step: 505 Acc: 0.46875\n",
      "Epoch Step: 506 Acc: 0.4375\n",
      "Epoch Step: 507 Acc: 0.5234375\n",
      "Epoch Step: 508 Acc: 0.53125\n",
      "Epoch Step: 509 Acc: 0.46875\n",
      "Epoch Step: 510 Acc: 0.4921875\n",
      "Epoch Step: 511 Acc: 0.4765625\n",
      "Epoch Step: 512 Acc: 0.4921875\n",
      "Epoch Step: 513 Acc: 0.4453125\n",
      "Epoch Step: 514 Acc: 0.46875\n",
      "Epoch Step: 515 Acc: 0.3984375\n",
      "Epoch Step: 516 Acc: 0.421875\n",
      "Epoch Step: 517 Acc: 0.4296875\n",
      "Epoch Step: 518 Acc: 0.4921875\n",
      "Epoch Step: 519 Acc: 0.484375\n",
      "Epoch Step: 520 Acc: 0.453125\n",
      "Epoch Step: 521 Acc: 0.375\n",
      "Epoch Step: 522 Acc: 0.4140625\n",
      "Epoch Step: 523 Acc: 0.4921875\n",
      "Epoch Step: 524 Acc: 0.484375\n",
      "Epoch Step: 525 Acc: 0.546875\n",
      "Epoch Step: 526 Acc: 0.5390625\n",
      "Epoch Step: 527 Acc: 0.5\n",
      "Epoch Step: 528 Acc: 0.4609375\n",
      "Epoch Step: 529 Acc: 0.46875\n",
      "Epoch Step: 530 Acc: 0.4921875\n",
      "Epoch Step: 531 Acc: 0.5078125\n",
      "Epoch Step: 532 Acc: 0.484375\n",
      "Epoch Step: 533 Acc: 0.5078125\n",
      "Epoch Step: 534 Acc: 0.4296875\n",
      "Epoch Step: 535 Acc: 0.4375\n",
      "Epoch Step: 536 Acc: 0.46875\n",
      "Epoch Step: 537 Acc: 0.4765625\n",
      "Epoch Step: 538 Acc: 0.46875\n",
      "Epoch Step: 539 Acc: 0.546875\n",
      "Epoch Step: 540 Acc: 0.5625\n",
      "Epoch Step: 541 Acc: 0.453125\n",
      "Epoch Step: 542 Acc: 0.5234375\n",
      "Epoch Step: 543 Acc: 0.5390625\n",
      "Epoch Step: 544 Acc: 0.484375\n",
      "Epoch Step: 545 Acc: 0.484375\n",
      "Epoch Step: 546 Acc: 0.53125\n",
      "Epoch Step: 547 Acc: 0.4453125\n",
      "Epoch Step: 548 Acc: 0.4453125\n",
      "Epoch Step: 549 Acc: 0.453125\n",
      "Epoch Step: 550 Acc: 0.484375\n",
      "Epoch Step: 551 Acc: 0.484375\n",
      "Epoch Step: 552 Acc: 0.546875\n",
      "Epoch Step: 553 Acc: 0.546875\n",
      "Epoch Step: 554 Acc: 0.484375\n",
      "Epoch Step: 555 Acc: 0.4765625\n",
      "Epoch Step: 556 Acc: 0.5\n",
      "Epoch Step: 557 Acc: 0.4765625\n",
      "Epoch Step: 558 Acc: 0.5234375\n",
      "Epoch Step: 559 Acc: 0.5234375\n",
      "Epoch Step: 560 Acc: 0.4453125\n",
      "Epoch Step: 561 Acc: 0.46875\n",
      "Epoch Step: 562 Acc: 0.4765625\n",
      "Epoch Step: 563 Acc: 0.484375\n",
      "Epoch Step: 564 Acc: 0.46875\n",
      "Epoch Step: 565 Acc: 0.5\n",
      "Epoch Step: 566 Acc: 0.5078125\n",
      "Epoch Step: 567 Acc: 0.546875\n",
      "Epoch Step: 568 Acc: 0.484375\n",
      "Epoch Step: 569 Acc: 0.4765625\n",
      "Epoch Step: 570 Acc: 0.4765625\n",
      "Epoch Step: 571 Acc: 0.515625\n",
      "Epoch Step: 572 Acc: 0.5234375\n",
      "Epoch Step: 573 Acc: 0.4453125\n",
      "Epoch Step: 574 Acc: 0.5\n",
      "Epoch Step: 575 Acc: 0.4921875\n",
      "Epoch Step: 576 Acc: 0.5078125\n",
      "Epoch Step: 577 Acc: 0.5\n",
      "Epoch Step: 578 Acc: 0.46875\n",
      "Epoch Step: 579 Acc: 0.53125\n",
      "Epoch Step: 580 Acc: 0.4921875\n",
      "Epoch Step: 581 Acc: 0.4296875\n",
      "Epoch Step: 582 Acc: 0.4765625\n",
      "Epoch Step: 583 Acc: 0.46875\n",
      "Epoch Step: 584 Acc: 0.515625\n",
      "Epoch Step: 585 Acc: 0.4296875\n",
      "Epoch Step: 586 Acc: 0.546875\n",
      "Epoch Step: 587 Acc: 0.5\n",
      "Epoch Step: 588 Acc: 0.46875\n",
      "Epoch Step: 589 Acc: 0.5078125\n",
      "Epoch Step: 590 Acc: 0.421875\n",
      "Epoch Step: 591 Acc: 0.5078125\n",
      "Epoch Step: 592 Acc: 0.5\n",
      "Epoch Step: 593 Acc: 0.5234375\n",
      "Epoch Step: 594 Acc: 0.4140625\n",
      "Epoch Step: 595 Acc: 0.4921875\n",
      "Epoch Step: 596 Acc: 0.515625\n",
      "Epoch Step: 597 Acc: 0.4921875\n",
      "Epoch Step: 598 Acc: 0.546875\n",
      "Epoch Step: 599 Acc: 0.4375\n",
      "Epoch Step: 600 Acc: 0.546875\n",
      "Epoch Step: 601 Acc: 0.4765625\n",
      "Epoch Step: 602 Acc: 0.5546875\n",
      "Epoch Step: 603 Acc: 0.421875\n",
      "Epoch Step: 604 Acc: 0.4765625\n",
      "Epoch Step: 605 Acc: 0.5\n",
      "Epoch Step: 606 Acc: 0.5546875\n",
      "Epoch Step: 607 Acc: 0.5078125\n",
      "Epoch Step: 608 Acc: 0.4609375\n",
      "Epoch Step: 609 Acc: 0.5078125\n",
      "Epoch Step: 610 Acc: 0.5234375\n",
      "Epoch Step: 611 Acc: 0.578125\n",
      "Epoch Step: 612 Acc: 0.5078125\n",
      "Epoch Step: 613 Acc: 0.46875\n",
      "Epoch Step: 614 Acc: 0.5078125\n",
      "Epoch Step: 615 Acc: 0.5\n",
      "Epoch Step: 616 Acc: 0.46875\n",
      "Epoch Step: 617 Acc: 0.46875\n",
      "Epoch Step: 618 Acc: 0.4453125\n",
      "Epoch Step: 619 Acc: 0.515625\n",
      "Epoch Step: 620 Acc: 0.53125\n",
      "Epoch Step: 621 Acc: 0.5390625\n",
      "Epoch Step: 622 Acc: 0.4140625\n",
      "Epoch Step: 623 Acc: 0.5\n",
      "Epoch Step: 624 Acc: 0.5625\n",
      "Epoch Step: 625 Acc: 0.4921875\n",
      "Epoch Step: 626 Acc: 0.4921875\n",
      "Epoch Step: 627 Acc: 0.484375\n",
      "Epoch Step: 628 Acc: 0.4921875\n",
      "Epoch Step: 629 Acc: 0.4921875\n",
      "Epoch Step: 630 Acc: 0.4453125\n",
      "Epoch Step: 631 Acc: 0.4296875\n",
      "Epoch Step: 632 Acc: 0.4375\n",
      "Epoch Step: 633 Acc: 0.5390625\n",
      "Epoch Step: 634 Acc: 0.5546875\n",
      "Epoch Step: 635 Acc: 0.3984375\n",
      "Epoch Step: 636 Acc: 0.484375\n",
      "Epoch Step: 637 Acc: 0.4140625\n",
      "Epoch Step: 638 Acc: 0.4296875\n",
      "Epoch Step: 639 Acc: 0.453125\n",
      "Epoch Step: 640 Acc: 0.5390625\n",
      "Epoch Step: 641 Acc: 0.5\n",
      "Epoch Step: 642 Acc: 0.546875\n",
      "Epoch Step: 643 Acc: 0.4765625\n",
      "Epoch Step: 644 Acc: 0.4609375\n",
      "Epoch Step: 645 Acc: 0.4765625\n",
      "Epoch Step: 646 Acc: 0.546875\n",
      "Epoch Step: 647 Acc: 0.484375\n",
      "Epoch Step: 648 Acc: 0.546875\n",
      "Epoch Step: 649 Acc: 0.421875\n",
      "Epoch Step: 650 Acc: 0.4609375\n",
      "Epoch Step: 651 Acc: 0.5625\n",
      "Epoch Step: 652 Acc: 0.5078125\n",
      "Epoch Step: 653 Acc: 0.46875\n",
      "Epoch Step: 654 Acc: 0.453125\n",
      "Epoch Step: 655 Acc: 0.5\n",
      "Epoch Step: 656 Acc: 0.5\n",
      "Epoch Step: 657 Acc: 0.4609375\n",
      "Epoch Step: 658 Acc: 0.5078125\n",
      "Epoch Step: 659 Acc: 0.4375\n",
      "Epoch Step: 660 Acc: 0.5078125\n",
      "Epoch Step: 661 Acc: 0.5703125\n",
      "Epoch Step: 662 Acc: 0.4765625\n",
      "Epoch Step: 663 Acc: 0.4453125\n",
      "Epoch Step: 664 Acc: 0.53125\n",
      "Epoch Step: 665 Acc: 0.484375\n",
      "Epoch Step: 666 Acc: 0.4765625\n",
      "Epoch Step: 667 Acc: 0.4609375\n",
      "Epoch Step: 668 Acc: 0.4921875\n",
      "Epoch Step: 669 Acc: 0.484375\n",
      "Epoch Step: 670 Acc: 0.4765625\n",
      "Epoch Step: 671 Acc: 0.515625\n",
      "Epoch Step: 672 Acc: 0.5\n",
      "Epoch Step: 673 Acc: 0.5078125\n",
      "Epoch Step: 674 Acc: 0.4453125\n",
      "Epoch Step: 675 Acc: 0.4375\n",
      "Epoch Step: 676 Acc: 0.46875\n",
      "Epoch Step: 677 Acc: 0.515625\n",
      "Epoch Step: 678 Acc: 0.4765625\n",
      "Epoch Step: 679 Acc: 0.4453125\n",
      "Epoch Step: 680 Acc: 0.5546875\n",
      "Epoch Step: 681 Acc: 0.328125\n",
      "Epoch Step: 682 Acc: 0.421875\n",
      "Epoch Step: 683 Acc: 0.5078125\n",
      "Epoch Step: 684 Acc: 0.515625\n",
      "Epoch Step: 685 Acc: 0.4296875\n",
      "Epoch Step: 686 Acc: 0.4140625\n",
      "Epoch Step: 687 Acc: 0.40625\n",
      "Epoch Step: 688 Acc: 0.4609375\n",
      "Epoch Step: 689 Acc: 0.4375\n",
      "Epoch Step: 690 Acc: 0.421875\n",
      "Epoch Step: 691 Acc: 0.4765625\n",
      "Epoch Step: 692 Acc: 0.484375\n",
      "Epoch Step: 693 Acc: 0.5234375\n",
      "Epoch Step: 694 Acc: 0.4921875\n",
      "Epoch Step: 695 Acc: 0.4296875\n",
      "Epoch Step: 696 Acc: 0.484375\n",
      "Epoch Step: 697 Acc: 0.5\n",
      "Epoch Step: 698 Acc: 0.484375\n",
      "Epoch Step: 699 Acc: 0.546875\n",
      "Epoch Step: 700 Acc: 0.46875\n",
      "Epoch Step: 701 Acc: 0.4765625\n",
      "Epoch Step: 702 Acc: 0.4609375\n",
      "Epoch Step: 703 Acc: 0.5\n",
      "Epoch Step: 704 Acc: 0.4140625\n",
      "Epoch Step: 705 Acc: 0.515625\n",
      "Epoch Step: 706 Acc: 0.5\n",
      "Epoch Step: 707 Acc: 0.4765625\n",
      "Epoch Step: 708 Acc: 0.5\n",
      "Epoch Step: 709 Acc: 0.5078125\n",
      "Epoch Step: 710 Acc: 0.4296875\n",
      "Epoch Step: 711 Acc: 0.4765625\n",
      "Epoch Step: 712 Acc: 0.4921875\n",
      "Epoch Step: 713 Acc: 0.4296875\n",
      "Epoch Step: 714 Acc: 0.3515625\n",
      "Epoch Step: 715 Acc: 0.40625\n",
      "Epoch Step: 716 Acc: 0.5546875\n",
      "Epoch Step: 717 Acc: 0.484375\n",
      "Epoch Step: 718 Acc: 0.4765625\n",
      "Epoch Step: 719 Acc: 0.5703125\n",
      "Epoch Step: 720 Acc: 0.421875\n",
      "Epoch Step: 721 Acc: 0.484375\n",
      "Epoch Step: 722 Acc: 0.421875\n",
      "Epoch Step: 723 Acc: 0.515625\n",
      "Epoch Step: 724 Acc: 0.484375\n",
      "Epoch Step: 725 Acc: 0.4609375\n",
      "Epoch Step: 726 Acc: 0.3828125\n",
      "Epoch Step: 727 Acc: 0.484375\n",
      "Epoch Step: 728 Acc: 0.4296875\n",
      "Epoch Step: 729 Acc: 0.3984375\n",
      "Epoch Step: 730 Acc: 0.4765625\n",
      "Epoch Step: 731 Acc: 0.46875\n",
      "Epoch Step: 732 Acc: 0.515625\n",
      "Epoch Step: 733 Acc: 0.4765625\n",
      "Epoch Step: 734 Acc: 0.4375\n",
      "Epoch Step: 735 Acc: 0.4375\n",
      "Epoch Step: 736 Acc: 0.515625\n",
      "Epoch Step: 737 Acc: 0.4609375\n",
      "Epoch Step: 738 Acc: 0.421875\n",
      "Epoch Step: 739 Acc: 0.4453125\n",
      "Epoch Step: 740 Acc: 0.5\n",
      "Epoch Step: 741 Acc: 0.4296875\n",
      "Epoch Step: 742 Acc: 0.4765625\n",
      "Epoch Step: 743 Acc: 0.3671875\n",
      "Epoch Step: 744 Acc: 0.5625\n",
      "Epoch Step: 745 Acc: 0.5234375\n",
      "Epoch Step: 746 Acc: 0.40625\n",
      "Epoch Step: 747 Acc: 0.4453125\n",
      "Epoch Step: 748 Acc: 0.5234375\n",
      "Epoch Step: 749 Acc: 0.4296875\n",
      "Epoch Step: 750 Acc: 0.3671875\n",
      "Epoch Step: 751 Acc: 0.46875\n",
      "Epoch Step: 752 Acc: 0.53125\n",
      "Epoch Step: 753 Acc: 0.4765625\n",
      "Epoch Step: 754 Acc: 0.5234375\n",
      "Epoch Step: 755 Acc: 0.4375\n",
      "Epoch Step: 756 Acc: 0.5234375\n",
      "Epoch Step: 757 Acc: 0.40625\n",
      "Epoch Step: 758 Acc: 0.515625\n",
      "Epoch Step: 759 Acc: 0.4921875\n",
      "Epoch Step: 760 Acc: 0.546875\n",
      "Epoch Step: 761 Acc: 0.4609375\n",
      "Epoch Step: 762 Acc: 0.5078125\n",
      "Epoch Step: 763 Acc: 0.4375\n",
      "Epoch Step: 764 Acc: 0.484375\n",
      "Epoch Step: 765 Acc: 0.5\n",
      "Epoch Step: 766 Acc: 0.4765625\n",
      "Epoch Step: 767 Acc: 0.4375\n",
      "Epoch Step: 768 Acc: 0.5\n",
      "Epoch Step: 769 Acc: 0.4375\n",
      "Epoch Step: 770 Acc: 0.4453125\n",
      "Epoch Step: 771 Acc: 0.4765625\n",
      "Epoch Step: 772 Acc: 0.4296875\n",
      "Epoch Step: 773 Acc: 0.5859375\n",
      "Epoch Step: 774 Acc: 0.3984375\n",
      "Epoch Step: 775 Acc: 0.515625\n",
      "Epoch Step: 776 Acc: 0.5234375\n",
      "Epoch Step: 777 Acc: 0.515625\n",
      "Epoch Step: 778 Acc: 0.453125\n",
      "Epoch Step: 779 Acc: 0.453125\n",
      "Epoch Step: 780 Acc: 0.484375\n",
      "Epoch Step: 781 Acc: 0.390625\n",
      "Epoch Step: 782 Acc: 0.40625\n",
      "Epoch Step: 783 Acc: 0.5234375\n",
      "Epoch Step: 784 Acc: 0.484375\n",
      "Epoch Step: 785 Acc: 0.5078125\n",
      "Epoch Step: 786 Acc: 0.5\n",
      "Epoch Step: 787 Acc: 0.484375\n",
      "Epoch Step: 788 Acc: 0.4453125\n",
      "Epoch Step: 789 Acc: 0.4453125\n",
      "Epoch Step: 790 Acc: 0.484375\n",
      "Epoch Step: 791 Acc: 0.4296875\n",
      "Epoch Step: 792 Acc: 0.3828125\n",
      "Epoch Step: 793 Acc: 0.3984375\n",
      "Epoch Step: 794 Acc: 0.5390625\n",
      "Epoch Step: 795 Acc: 0.5234375\n",
      "Epoch Step: 796 Acc: 0.515625\n",
      "Epoch Step: 797 Acc: 0.546875\n",
      "Epoch Step: 798 Acc: 0.484375\n",
      "Epoch Step: 799 Acc: 0.421875\n",
      "Epoch Step: 800 Acc: 0.453125\n",
      "Epoch Step: 801 Acc: 0.5078125\n",
      "Epoch Step: 802 Acc: 0.5078125\n",
      "Epoch Step: 803 Acc: 0.4296875\n",
      "Epoch Step: 804 Acc: 0.484375\n",
      "Epoch Step: 805 Acc: 0.4609375\n",
      "Epoch Step: 806 Acc: 0.46875\n",
      "Epoch Step: 807 Acc: 0.359375\n",
      "Epoch Step: 808 Acc: 0.484375\n",
      "Epoch Step: 809 Acc: 0.4296875\n",
      "Epoch Step: 810 Acc: 0.4921875\n",
      "Epoch Step: 811 Acc: 0.4765625\n",
      "Epoch Step: 812 Acc: 0.5390625\n",
      "Epoch Step: 813 Acc: 0.46875\n",
      "Epoch Step: 814 Acc: 0.453125\n",
      "Epoch Step: 815 Acc: 0.4765625\n",
      "Epoch Step: 816 Acc: 0.4375\n",
      "Epoch Step: 817 Acc: 0.4921875\n",
      "Epoch Step: 818 Acc: 0.4921875\n",
      "Epoch Step: 819 Acc: 0.4296875\n",
      "Epoch Step: 820 Acc: 0.5234375\n",
      "Epoch Step: 821 Acc: 0.5390625\n",
      "Epoch Step: 822 Acc: 0.46875\n",
      "Epoch Step: 823 Acc: 0.46875\n",
      "Epoch Step: 824 Acc: 0.46875\n",
      "Epoch Step: 825 Acc: 0.4453125\n",
      "Epoch Step: 826 Acc: 0.421875\n",
      "Epoch Step: 827 Acc: 0.59375\n",
      "Epoch Step: 828 Acc: 0.5\n",
      "Epoch Step: 829 Acc: 0.5\n",
      "Epoch Step: 830 Acc: 0.484375\n",
      "Epoch Step: 831 Acc: 0.4453125\n",
      "Epoch Step: 832 Acc: 0.4765625\n",
      "Epoch Step: 833 Acc: 0.515625\n",
      "Epoch Step: 834 Acc: 0.4375\n",
      "Epoch Step: 835 Acc: 0.40625\n",
      "Epoch Step: 836 Acc: 0.46875\n",
      "Epoch Step: 837 Acc: 0.515625\n",
      "Epoch Step: 838 Acc: 0.46875\n",
      "Epoch Step: 839 Acc: 0.46875\n",
      "Epoch Step: 840 Acc: 0.5390625\n",
      "Epoch Step: 841 Acc: 0.4453125\n",
      "Epoch Step: 842 Acc: 0.5078125\n",
      "Epoch Step: 843 Acc: 0.453125\n",
      "Epoch Step: 844 Acc: 0.5546875\n",
      "Epoch Step: 845 Acc: 0.4609375\n",
      "Epoch Step: 846 Acc: 0.546875\n",
      "Epoch Step: 847 Acc: 0.515625\n",
      "Epoch Step: 848 Acc: 0.4609375\n",
      "Epoch Step: 849 Acc: 0.4375\n",
      "Epoch Step: 850 Acc: 0.421875\n",
      "Epoch Step: 851 Acc: 0.484375\n",
      "Epoch Step: 852 Acc: 0.484375\n",
      "Epoch Step: 853 Acc: 0.46875\n",
      "Epoch Step: 854 Acc: 0.375\n",
      "Epoch Step: 855 Acc: 0.6015625\n",
      "Epoch Step: 856 Acc: 0.53125\n",
      "Epoch Step: 857 Acc: 0.5\n",
      "Epoch Step: 858 Acc: 0.5\n",
      "Epoch Step: 859 Acc: 0.484375\n",
      "Epoch Step: 860 Acc: 0.5078125\n",
      "Epoch Step: 861 Acc: 0.4921875\n",
      "Epoch Step: 862 Acc: 0.484375\n",
      "Epoch Step: 863 Acc: 0.515625\n",
      "Epoch Step: 864 Acc: 0.4375\n",
      "Epoch Step: 865 Acc: 0.4609375\n",
      "Epoch Step: 866 Acc: 0.5\n",
      "Epoch Step: 867 Acc: 0.5234375\n",
      "Epoch Step: 868 Acc: 0.4609375\n",
      "Epoch Step: 869 Acc: 0.4296875\n",
      "Epoch Step: 870 Acc: 0.375\n",
      "Epoch Step: 871 Acc: 0.3828125\n",
      "Epoch Step: 872 Acc: 0.484375\n",
      "Epoch Step: 873 Acc: 0.53125\n",
      "Epoch Step: 874 Acc: 0.578125\n",
      "Epoch Step: 875 Acc: 0.5625\n",
      "Epoch Step: 876 Acc: 0.46875\n",
      "Epoch Step: 877 Acc: 0.546875\n",
      "Epoch Step: 878 Acc: 0.53125\n",
      "Epoch Step: 879 Acc: 0.4765625\n",
      "Epoch Step: 880 Acc: 0.46875\n",
      "Epoch Step: 881 Acc: 0.5\n",
      "Epoch Step: 882 Acc: 0.4375\n",
      "Epoch Step: 883 Acc: 0.4609375\n",
      "Epoch Step: 884 Acc: 0.421875\n",
      "Epoch Step: 885 Acc: 0.4453125\n",
      "Epoch Step: 886 Acc: 0.5390625\n",
      "Epoch Step: 887 Acc: 0.515625\n",
      "Epoch Step: 888 Acc: 0.5390625\n",
      "Epoch Step: 889 Acc: 0.484375\n",
      "Epoch Step: 890 Acc: 0.4609375\n",
      "Epoch Step: 891 Acc: 0.421875\n",
      "Epoch Step: 892 Acc: 0.4765625\n",
      "Epoch Step: 893 Acc: 0.46875\n",
      "Epoch Step: 894 Acc: 0.5078125\n",
      "Epoch Step: 895 Acc: 0.5078125\n",
      "Epoch Step: 896 Acc: 0.453125\n",
      "Epoch Step: 897 Acc: 0.515625\n",
      "Epoch Step: 898 Acc: 0.5078125\n",
      "Epoch Step: 899 Acc: 0.5390625\n",
      "Epoch Step: 900 Acc: 0.4921875\n",
      "Epoch Step: 901 Acc: 0.515625\n",
      "Epoch Step: 902 Acc: 0.4609375\n",
      "Epoch Step: 903 Acc: 0.5\n",
      "Epoch Step: 904 Acc: 0.453125\n",
      "Epoch Step: 905 Acc: 0.4765625\n",
      "Epoch Step: 906 Acc: 0.5\n",
      "Epoch Step: 907 Acc: 0.46875\n",
      "Epoch Step: 908 Acc: 0.4921875\n",
      "Epoch Step: 909 Acc: 0.484375\n",
      "Epoch Step: 910 Acc: 0.5078125\n",
      "Epoch Step: 911 Acc: 0.5\n",
      "Epoch Step: 912 Acc: 0.421875\n",
      "Epoch Step: 913 Acc: 0.5625\n",
      "Epoch Step: 914 Acc: 0.515625\n",
      "Epoch Step: 915 Acc: 0.5\n",
      "Epoch Step: 916 Acc: 0.53125\n",
      "Epoch Step: 917 Acc: 0.4765625\n",
      "Epoch Step: 918 Acc: 0.5546875\n",
      "Epoch Step: 919 Acc: 0.46875\n",
      "Epoch Step: 920 Acc: 0.5078125\n",
      "Epoch Step: 921 Acc: 0.4609375\n",
      "Epoch Step: 922 Acc: 0.453125\n",
      "Epoch Step: 923 Acc: 0.4140625\n",
      "Epoch Step: 924 Acc: 0.375\n",
      "Epoch Step: 925 Acc: 0.5390625\n",
      "Epoch Step: 926 Acc: 0.46875\n",
      "Epoch Step: 927 Acc: 0.4921875\n",
      "Epoch Step: 928 Acc: 0.4140625\n",
      "Epoch Step: 929 Acc: 0.40625\n",
      "Epoch Step: 930 Acc: 0.40625\n",
      "Epoch Step: 931 Acc: 0.5078125\n",
      "Epoch Step: 932 Acc: 0.5078125\n",
      "Epoch Step: 933 Acc: 0.390625\n",
      "Epoch Step: 934 Acc: 0.5078125\n",
      "Epoch Step: 935 Acc: 0.4609375\n",
      "Epoch Step: 936 Acc: 0.5234375\n",
      "Epoch Step: 937 Acc: 0.4140625\n",
      "Epoch Step: 938 Acc: 0.4375\n",
      "Epoch Step: 939 Acc: 0.4921875\n",
      "Epoch Step: 940 Acc: 0.4375\n",
      "Epoch Step: 941 Acc: 0.484375\n",
      "Epoch Step: 942 Acc: 0.453125\n",
      "Epoch Step: 943 Acc: 0.4375\n",
      "Epoch Step: 944 Acc: 0.4765625\n",
      "Epoch Step: 945 Acc: 0.4453125\n",
      "Epoch Step: 946 Acc: 0.4765625\n",
      "Epoch Step: 947 Acc: 0.5234375\n",
      "Epoch Step: 948 Acc: 0.5\n",
      "Epoch Step: 949 Acc: 0.5234375\n",
      "Epoch Step: 950 Acc: 0.46875\n",
      "Epoch Step: 951 Acc: 0.5390625\n",
      "Epoch Step: 952 Acc: 0.4921875\n",
      "Epoch Step: 953 Acc: 0.46875\n",
      "Epoch Step: 954 Acc: 0.4296875\n",
      "Epoch Step: 955 Acc: 0.484375\n",
      "Epoch Step: 956 Acc: 0.4921875\n",
      "Epoch Step: 957 Acc: 0.4765625\n",
      "Epoch Step: 958 Acc: 0.4375\n",
      "Epoch Step: 959 Acc: 0.484375\n",
      "Epoch Step: 960 Acc: 0.515625\n",
      "Epoch Step: 961 Acc: 0.4609375\n",
      "Epoch Step: 962 Acc: 0.4296875\n",
      "Epoch Step: 963 Acc: 0.4921875\n",
      "Epoch Step: 964 Acc: 0.4921875\n",
      "Epoch Step: 965 Acc: 0.5078125\n",
      "Epoch Step: 966 Acc: 0.46875\n",
      "Epoch Step: 967 Acc: 0.4609375\n",
      "Epoch Step: 968 Acc: 0.5\n",
      "Epoch Step: 969 Acc: 0.4296875\n",
      "Epoch Step: 970 Acc: 0.515625\n",
      "Epoch Step: 971 Acc: 0.484375\n",
      "Epoch Step: 972 Acc: 0.4609375\n",
      "Epoch Step: 973 Acc: 0.546875\n",
      "Epoch Step: 974 Acc: 0.4140625\n",
      "Epoch Step: 975 Acc: 0.46875\n",
      "Epoch Step: 976 Acc: 0.4296875\n",
      "Epoch Step: 977 Acc: 0.4921875\n",
      "Epoch Step: 978 Acc: 0.46875\n",
      "Epoch Step: 979 Acc: 0.421875\n",
      "Epoch Step: 980 Acc: 0.453125\n",
      "Epoch Step: 981 Acc: 0.4453125\n",
      "Epoch Step: 982 Acc: 0.453125\n",
      "Epoch Step: 983 Acc: 0.4453125\n",
      "Epoch Step: 984 Acc: 0.4609375\n",
      "Epoch Step: 985 Acc: 0.515625\n",
      "Epoch Step: 986 Acc: 0.5078125\n",
      "Epoch Step: 987 Acc: 0.5234375\n",
      "Epoch Step: 988 Acc: 0.4609375\n",
      "Epoch Step: 989 Acc: 0.4375\n",
      "Epoch Step: 990 Acc: 0.4609375\n",
      "Epoch Step: 991 Acc: 0.4921875\n",
      "Epoch Step: 992 Acc: 0.53125\n",
      "Epoch Step: 993 Acc: 0.3671875\n",
      "Epoch Step: 994 Acc: 0.4296875\n",
      "Epoch Step: 995 Acc: 0.4609375\n",
      "Epoch Step: 996 Acc: 0.4921875\n",
      "Epoch Step: 997 Acc: 0.4453125\n",
      "Epoch Step: 998 Acc: 0.5234375\n",
      "Epoch Step: 999 Acc: 0.46875\n",
      "Epoch Step: 1000 Acc: 0.484375\n",
      "Epoch Step: 1001 Acc: 0.4765625\n",
      "Epoch Step: 1002 Acc: 0.453125\n",
      "Epoch Step: 1003 Acc: 0.5\n",
      "Epoch Step: 1004 Acc: 0.4765625\n",
      "Epoch Step: 1005 Acc: 0.53125\n",
      "Epoch Step: 1006 Acc: 0.5\n",
      "Epoch Step: 1007 Acc: 0.4375\n",
      "Epoch Step: 1008 Acc: 0.5234375\n",
      "Epoch Step: 1009 Acc: 0.546875\n",
      "Epoch Step: 1010 Acc: 0.4296875\n",
      "Epoch Step: 1011 Acc: 0.4609375\n",
      "Epoch Step: 1012 Acc: 0.5703125\n",
      "Epoch Step: 1013 Acc: 0.4140625\n",
      "Epoch Step: 1014 Acc: 0.4453125\n",
      "Epoch Step: 1015 Acc: 0.5234375\n",
      "Epoch Step: 1016 Acc: 0.5234375\n",
      "Epoch Step: 1017 Acc: 0.5078125\n",
      "Epoch Step: 1018 Acc: 0.5234375\n",
      "Epoch Step: 1019 Acc: 0.3984375\n",
      "Epoch Step: 1020 Acc: 0.453125\n",
      "Epoch Step: 1021 Acc: 0.4921875\n",
      "Epoch Step: 1022 Acc: 0.53125\n",
      "Epoch Step: 1023 Acc: 0.4765625\n",
      "Epoch Step: 1024 Acc: 0.46875\n",
      "Epoch Step: 1025 Acc: 0.4921875\n",
      "Epoch Step: 1026 Acc: 0.5234375\n",
      "Epoch Step: 1027 Acc: 0.453125\n",
      "Epoch Step: 1028 Acc: 0.5\n",
      "Epoch Step: 1029 Acc: 0.515625\n",
      "Epoch Step: 1030 Acc: 0.5390625\n",
      "Epoch Step: 1031 Acc: 0.4921875\n",
      "Epoch Step: 1032 Acc: 0.53125\n",
      "Epoch Step: 1033 Acc: 0.5390625\n",
      "Epoch Step: 1034 Acc: 0.453125\n",
      "Epoch Step: 1035 Acc: 0.4609375\n",
      "Epoch Step: 1036 Acc: 0.5078125\n",
      "Epoch Step: 1037 Acc: 0.46875\n",
      "Epoch Step: 1038 Acc: 0.515625\n",
      "Epoch Step: 1039 Acc: 0.4140625\n",
      "Epoch Step: 1040 Acc: 0.453125\n",
      "Epoch Step: 1041 Acc: 0.4375\n",
      "Epoch Step: 1042 Acc: 0.4609375\n",
      "Epoch Step: 1043 Acc: 0.46875\n",
      "Epoch Step: 1044 Acc: 0.5625\n",
      "Epoch Step: 1045 Acc: 0.40625\n",
      "Epoch Step: 1046 Acc: 0.4921875\n",
      "Epoch Step: 1047 Acc: 0.5546875\n",
      "Epoch Step: 1048 Acc: 0.4453125\n",
      "Epoch Step: 1049 Acc: 0.453125\n",
      "Epoch Step: 1050 Acc: 0.515625\n",
      "Epoch Step: 1051 Acc: 0.4140625\n",
      "Epoch Step: 1052 Acc: 0.4140625\n",
      "Epoch Step: 1053 Acc: 0.4453125\n",
      "Epoch Step: 1054 Acc: 0.4296875\n",
      "Epoch Step: 1055 Acc: 0.4453125\n",
      "Epoch Step: 1056 Acc: 0.421875\n",
      "Epoch Step: 1057 Acc: 0.421875\n",
      "Epoch Step: 1058 Acc: 0.3671875\n",
      "Epoch Step: 1059 Acc: 0.53125\n",
      "Epoch Step: 1060 Acc: 0.5546875\n",
      "Epoch Step: 1061 Acc: 0.46875\n",
      "Epoch Step: 1062 Acc: 0.5\n",
      "Epoch Step: 1063 Acc: 0.53125\n",
      "Epoch Step: 1064 Acc: 0.4765625\n",
      "Epoch Step: 1065 Acc: 0.46875\n",
      "Epoch Step: 1066 Acc: 0.4921875\n",
      "Epoch Step: 1067 Acc: 0.484375\n",
      "Epoch Step: 1068 Acc: 0.5\n",
      "Epoch Step: 1069 Acc: 0.5234375\n",
      "Epoch Step: 1070 Acc: 0.4453125\n",
      "Epoch Step: 1071 Acc: 0.4765625\n",
      "Epoch Step: 1072 Acc: 0.4375\n",
      "Epoch Step: 1073 Acc: 0.4453125\n",
      "Epoch Step: 1074 Acc: 0.4609375\n",
      "Epoch Step: 1075 Acc: 0.4609375\n",
      "Epoch Step: 1076 Acc: 0.5625\n",
      "Epoch Step: 1077 Acc: 0.4921875\n",
      "Epoch Step: 1078 Acc: 0.5234375\n",
      "Epoch Step: 1079 Acc: 0.4453125\n",
      "Epoch Step: 1080 Acc: 0.5078125\n",
      "Epoch Step: 1081 Acc: 0.4375\n",
      "Epoch Step: 1082 Acc: 0.4765625\n",
      "Epoch Step: 1083 Acc: 0.4453125\n",
      "Epoch Step: 1084 Acc: 0.46875\n",
      "Epoch Step: 1085 Acc: 0.3984375\n",
      "Epoch Step: 1086 Acc: 0.453125\n",
      "Epoch Step: 1087 Acc: 0.4453125\n",
      "Epoch Step: 1088 Acc: 0.53125\n",
      "Epoch Step: 1089 Acc: 0.4453125\n",
      "Epoch Step: 1090 Acc: 0.4296875\n",
      "Epoch Step: 1091 Acc: 0.4609375\n",
      "Epoch Step: 1092 Acc: 0.484375\n",
      "Epoch Step: 1093 Acc: 0.4453125\n",
      "Epoch Step: 1094 Acc: 0.4921875\n",
      "Epoch Step: 1095 Acc: 0.53125\n",
      "Epoch Step: 1096 Acc: 0.4921875\n",
      "Epoch Step: 1097 Acc: 0.453125\n",
      "Epoch Step: 1098 Acc: 0.46875\n",
      "Epoch Step: 1099 Acc: 0.4921875\n",
      "Epoch Step: 1100 Acc: 0.3984375\n",
      "Epoch Step: 1101 Acc: 0.4296875\n",
      "Epoch Step: 1102 Acc: 0.4140625\n",
      "Epoch Step: 1103 Acc: 0.4609375\n",
      "Epoch Step: 1104 Acc: 0.546875\n",
      "Epoch Step: 1105 Acc: 0.5078125\n",
      "Epoch Step: 1106 Acc: 0.4921875\n",
      "Epoch Step: 1107 Acc: 0.484375\n",
      "Epoch Step: 1108 Acc: 0.46875\n",
      "Epoch Step: 1109 Acc: 0.40625\n",
      "Epoch Step: 1110 Acc: 0.5\n",
      "Epoch Step: 1111 Acc: 0.5234375\n",
      "Epoch Step: 1112 Acc: 0.4921875\n",
      "Epoch Step: 1113 Acc: 0.4453125\n",
      "Epoch Step: 1114 Acc: 0.4296875\n",
      "Epoch Step: 1115 Acc: 0.4609375\n",
      "Epoch Step: 1116 Acc: 0.5078125\n",
      "Epoch Step: 1117 Acc: 0.4296875\n",
      "Epoch Step: 1118 Acc: 0.40625\n",
      "Epoch Step: 1119 Acc: 0.4765625\n",
      "Epoch Step: 1120 Acc: 0.484375\n",
      "Epoch Step: 1121 Acc: 0.53125\n",
      "Epoch Step: 1122 Acc: 0.484375\n",
      "Epoch Step: 1123 Acc: 0.4765625\n",
      "Epoch Step: 1124 Acc: 0.5078125\n",
      "Epoch Step: 1125 Acc: 0.4375\n",
      "Epoch Step: 1126 Acc: 0.4765625\n",
      "Epoch Step: 1127 Acc: 0.484375\n",
      "Epoch Step: 1128 Acc: 0.4921875\n",
      "Epoch Step: 1129 Acc: 0.4921875\n",
      "Epoch Step: 1130 Acc: 0.5234375\n",
      "Epoch Step: 1131 Acc: 0.5\n",
      "Epoch Step: 1132 Acc: 0.53125\n",
      "Epoch Step: 1133 Acc: 0.4453125\n",
      "Epoch Step: 1134 Acc: 0.4921875\n",
      "Epoch Step: 1135 Acc: 0.5546875\n",
      "Epoch Step: 1136 Acc: 0.53125\n",
      "Epoch Step: 1137 Acc: 0.5\n",
      "Epoch Step: 1138 Acc: 0.421875\n",
      "Epoch Step: 1139 Acc: 0.5546875\n",
      "Epoch Step: 1140 Acc: 0.5\n",
      "Epoch Step: 1141 Acc: 0.515625\n",
      "Epoch Step: 1142 Acc: 0.5546875\n",
      "Epoch Step: 1143 Acc: 0.5078125\n",
      "Epoch Step: 1144 Acc: 0.578125\n",
      "Epoch Step: 1145 Acc: 0.5546875\n",
      "Epoch Step: 1146 Acc: 0.515625\n",
      "Epoch Step: 1147 Acc: 0.4921875\n",
      "Epoch Step: 1148 Acc: 0.484375\n",
      "Epoch Step: 1149 Acc: 0.484375\n",
      "Epoch Step: 1150 Acc: 0.3984375\n",
      "Epoch Step: 1151 Acc: 0.5390625\n",
      "Epoch Step: 1152 Acc: 0.4140625\n",
      "Epoch Step: 1153 Acc: 0.5\n",
      "Epoch Step: 1154 Acc: 0.484375\n",
      "Epoch Step: 1155 Acc: 0.4375\n",
      "Epoch Step: 1156 Acc: 0.4375\n",
      "Epoch Step: 1157 Acc: 0.5078125\n",
      "Epoch Step: 1158 Acc: 0.5390625\n",
      "Epoch Step: 1159 Acc: 0.4609375\n",
      "Epoch Step: 1160 Acc: 0.4140625\n",
      "Epoch Step: 1161 Acc: 0.40625\n",
      "Epoch Step: 1162 Acc: 0.46875\n",
      "Epoch Step: 1163 Acc: 0.4375\n",
      "Epoch Step: 1164 Acc: 0.421875\n",
      "Epoch Step: 1165 Acc: 0.4453125\n",
      "Epoch Step: 1166 Acc: 0.46875\n",
      "Epoch Step: 1167 Acc: 0.4609375\n",
      "Epoch Step: 1168 Acc: 0.453125\n",
      "Epoch Step: 1169 Acc: 0.46875\n",
      "Epoch Step: 1170 Acc: 0.3828125\n",
      "Epoch Step: 1171 Acc: 0.4765625\n",
      "Epoch Step: 1172 Acc: 0.5078125\n",
      "Epoch Step: 1173 Acc: 0.484375\n",
      "Epoch Step: 1174 Acc: 0.4296875\n",
      "Epoch Step: 1175 Acc: 0.484375\n",
      "Epoch Step: 1176 Acc: 0.515625\n",
      "Epoch Step: 1177 Acc: 0.5\n",
      "Epoch Step: 1178 Acc: 0.4765625\n",
      "Epoch Step: 1179 Acc: 0.6015625\n",
      "Epoch Step: 1180 Acc: 0.4921875\n",
      "Epoch Step: 1181 Acc: 0.515625\n",
      "Epoch Step: 1182 Acc: 0.453125\n",
      "Epoch Step: 1183 Acc: 0.4765625\n",
      "Epoch Step: 1184 Acc: 0.53125\n",
      "Epoch Step: 1185 Acc: 0.5234375\n",
      "Epoch Step: 1186 Acc: 0.4765625\n",
      "Epoch Step: 1187 Acc: 0.453125\n",
      "Epoch Step: 1188 Acc: 0.515625\n",
      "Epoch Step: 1189 Acc: 0.390625\n",
      "Epoch Step: 1190 Acc: 0.453125\n",
      "Epoch Step: 1191 Acc: 0.4921875\n",
      "Epoch Step: 1192 Acc: 0.6015625\n",
      "Epoch Step: 1193 Acc: 0.5625\n",
      "Epoch Step: 1194 Acc: 0.46875\n",
      "Epoch Step: 1195 Acc: 0.4296875\n",
      "Epoch Step: 1196 Acc: 0.4609375\n",
      "Epoch Step: 1197 Acc: 0.390625\n",
      "Epoch Step: 1198 Acc: 0.421875\n",
      "Epoch Step: 1199 Acc: 0.5234375\n",
      "Epoch Step: 1200 Acc: 0.5\n",
      "Epoch Step: 1201 Acc: 0.4609375\n",
      "Epoch Step: 1202 Acc: 0.4375\n",
      "Epoch Step: 1203 Acc: 0.46875\n",
      "Epoch Step: 1204 Acc: 0.53125\n",
      "Epoch Step: 1205 Acc: 0.5078125\n",
      "Epoch Step: 1206 Acc: 0.4765625\n",
      "Epoch Step: 1207 Acc: 0.5546875\n",
      "Epoch Step: 1208 Acc: 0.4765625\n",
      "Epoch Step: 1209 Acc: 0.4453125\n",
      "Epoch Step: 1210 Acc: 0.4921875\n",
      "Epoch Step: 1211 Acc: 0.4296875\n",
      "Epoch Step: 1212 Acc: 0.453125\n",
      "Epoch Step: 1213 Acc: 0.4296875\n",
      "Epoch Step: 1214 Acc: 0.5546875\n",
      "Epoch Step: 1215 Acc: 0.609375\n",
      "Epoch Step: 1216 Acc: 0.46875\n",
      "Epoch Step: 1217 Acc: 0.5\n",
      "Epoch Step: 1218 Acc: 0.5703125\n",
      "Epoch Step: 1219 Acc: 0.4921875\n",
      "Epoch Step: 1220 Acc: 0.484375\n",
      "Epoch Step: 1221 Acc: 0.4140625\n",
      "Epoch Step: 1222 Acc: 0.5\n",
      "Epoch Step: 1223 Acc: 0.484375\n",
      "Epoch Step: 1224 Acc: 0.484375\n",
      "Epoch Step: 1225 Acc: 0.5234375\n",
      "Epoch Step: 1226 Acc: 0.40625\n",
      "Epoch Step: 1227 Acc: 0.5390625\n",
      "Epoch Step: 1228 Acc: 0.4921875\n",
      "Epoch Step: 1229 Acc: 0.421875\n",
      "Epoch Step: 1230 Acc: 0.40625\n",
      "Epoch Step: 1231 Acc: 0.5078125\n",
      "Epoch Step: 1232 Acc: 0.6015625\n",
      "Epoch Step: 1233 Acc: 0.4140625\n",
      "Epoch Step: 1234 Acc: 0.46875\n",
      "Epoch Step: 1235 Acc: 0.484375\n",
      "Epoch Step: 1236 Acc: 0.5078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tgt \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m logits \u001b[39m=\u001b[39m trained_model(src_ids, src_len)\n\u001b[1;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch Step: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m Acc: \u001b[39m\u001b[39m{\u001b[39;00m(logits\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m tgt)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch, (src_ids, src_len, tgt) in enumerate(train_dataloader):\n",
    "    src_ids = src_ids.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    logits = trained_model(src_ids, src_len)\n",
    "    print(f\"Epoch Step: {batch} Acc: {(logits.argmax(1) == tgt).sum().item() / tgt.size(0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ecdeeea834996ab17c476b9c82385c086ccb21d059ebd3afa1913627f92baf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
