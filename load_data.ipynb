{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Downloaded from https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/dataset/dataset_blog.json\n",
    "with open(\"blog.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "docs = json_data['docs'][1:] # I don't want to see the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19676/19676 [00:20<00:00, 982.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 559126\n",
      "Counting freqeuncies of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 559126/559126 [00:10<00:00, 53425.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with lengths <= 128: 554016\n",
      "Number of unique words before converting to <UNK>:  505954\n",
      "Converting words with frequencies less than 5 to <UNK>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "554016it [00:09, 59662.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting <UNK>:  85906\n",
      "Known occurrences rate 99.01%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Took me 2 min to run\n",
    "\n",
    "def stop(limit = 1, times = [0]):\n",
    "    times[0] += 1\n",
    "    assert times[0] < limit, \"STOP HERE\"\n",
    "\n",
    "\n",
    "class GenderDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, docs, PARAGRAPH_LENGTH = 128, UNK_THRESHOLD = 5) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        SEP_TOKEN = \"<SEP>\"\n",
    "        SOS_TOKEN = \"<SOS>\" # This is a misnomer, it is actually the first token of the paragraph\n",
    "        EOS_TOKEN = \"<EOS>\" # This is a misnomer, it is actually the last token of the paragraph\n",
    "\n",
    "        def combine_and_cut(rawtext):\n",
    "            \"\"\"\n",
    "            Generates finer grained sentences by splitting on punctuation.\n",
    "            Used to cut the document into paragraphs.\n",
    "\n",
    "            Input: list of strings\n",
    "            Output: Generate sentences one by one\n",
    "            \"\"\"\n",
    "            for sent in rawtext:\n",
    "                # Split on punctuation\n",
    "                sent = re.split(r' ([.!?])', sent)\n",
    "                # Remove punctuation at the beginning and end of sentences\n",
    "                sent = [re.sub(r'^[.!?]', '', s) for s in sent]\n",
    "                sent = [re.sub(r'[.!?]$', '', s) for s in sent]\n",
    "                # Remove empty strings and convert to lower case\n",
    "                sent = [x.lower() for x in sent if x]\n",
    "                yield from sent\n",
    "        \n",
    "        #---------------------------------------------------------------------------   \n",
    "        #  Cut the documents into paragraphs of PARAGRAPH_LENGTH\n",
    "        #---------------------------------------------------------------------------     \n",
    "        doc_texts = [] # List of paragraphs\n",
    "        gender_label = []\n",
    "\n",
    "        print(f\"Cutting documents into paragraphs of length {PARAGRAPH_LENGTH}...\")\n",
    "        for doc in tqdm(docs):\n",
    "\n",
    "            gender = int(doc['gender'] == 'male')\n",
    "            doc_texts.append(SOS_TOKEN)\n",
    "            gender_label.append(gender)\n",
    "\n",
    "            for sent in combine_and_cut(doc['rawtext']):\n",
    "                if len(doc_texts[-1].split()) + len(sent.split()) > PARAGRAPH_LENGTH - 2: # Minus 2 for SEP and EOS tokens\n",
    "                    doc_texts[-1] += \" \" + EOS_TOKEN\n",
    "                    doc_texts.append(SOS_TOKEN)\n",
    "                    gender_label.append(gender)\n",
    "                doc_texts[-1] += sent + \" \" + SEP_TOKEN\n",
    "            doc_texts[-1] += \" \" + EOS_TOKEN\n",
    "        \n",
    "        print(f\"Number of documents: {len(doc_texts)}\")\n",
    "\n",
    "        print(f\"Counting freqeuncies of words...\")\n",
    "        freq = Counter() # Count the number of times each word appears\n",
    "        doc_texts_token = [] # This is a list of lists of tokens\n",
    "        for doc_text in tqdm(doc_texts):\n",
    "            if len(doc_text.split()) <= PARAGRAPH_LENGTH:\n",
    "                doc_text_token = doc_text.split()\n",
    "                freq.update(doc_text_token)\n",
    "                doc_texts_token.append(doc_text_token)\n",
    "        \n",
    "        print(f\"Number of documents with lengths <= {PARAGRAPH_LENGTH}: {len(doc_texts_token)}\")\n",
    "\n",
    "        #---------------------------------------------------------------------------   \n",
    "        #  Convert words to <UNK>, then to indices\n",
    "        #---------------------------------------------------------------------------     \n",
    "        print(\"Number of unique words before converting to <UNK>: \", len(freq))\n",
    "        before_occur = sum(freq.values())\n",
    "\n",
    "        unique_words = set()\n",
    "\n",
    "        print(f\"Converting words with frequencies less than {UNK_THRESHOLD} to <UNK>...\")\n",
    "        total_occur = before_occur\n",
    "        for i, doc_text_token in tqdm(enumerate(doc_texts_token)):\n",
    "            # Replace words with less than 5 occurrences with <UNK>\n",
    "            doc_text_token = [word if freq[word] > UNK_THRESHOLD else \"<UNK>\" for word in doc_text_token]\n",
    "            unique_words.update(doc_text_token)\n",
    "            total_occur -= doc_text_token.count(\"<UNK>\")\n",
    "            doc_texts_token[i] = doc_text_token\n",
    "        print(\"Number of unique words after converting <UNK>: \", len(unique_words))\n",
    "        print(f\"Known occurrences rate {round(total_occur/before_occur * 100, 2)}%\")\n",
    "        \n",
    "        self.raw_text = doc_texts # list of strings of ~128 words\n",
    "        self.vocab_size = len(unique_words) # numbers of unique words == len(token2idx)\n",
    "        self.vocab = unique_words # set of unique words\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(unique_words)} # dict of unique words to indices\n",
    "        self.idx2token = {idx: token for idx, token in enumerate(unique_words)} # dict of indices to unique words\n",
    "        self.data = [[self.token2idx[token] for token in doc] for doc in doc_texts_token]\n",
    "        self.label = gender_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx in range(len(self)):\n",
    "            yield self[idx]\n",
    "    \n",
    "\n",
    "    def tokenize(self, idx):\n",
    "        \"\"\"\n",
    "        Converts indice or a list of indices to tokens.\n",
    "        Input: int or list of ints\n",
    "        Output: string or list of strings\n",
    "        \"\"\"\n",
    "        if isinstance(idx, int):\n",
    "            return self.idx2token[idx]\n",
    "        elif isinstance(idx, list):\n",
    "            return \" \".join([self.idx2token[i] for i in idx])\n",
    "        else:\n",
    "            raise TypeError(f\"Expected int or list, got {type(idx)}\")\n",
    "    \n",
    "    def detokenize(self, token):\n",
    "        \"\"\"\n",
    "        Converts token or a list of tokens to indices.\n",
    "        Input: string. Document/paragraph to be converted to indices\n",
    "        Output: list of ints\n",
    "        \"\"\"\n",
    "        if isinstance(token, str):\n",
    "            return [self.token2idx[t] for t in token.split()]\n",
    "        else:\n",
    "            raise TypeError(f\"Expected str, got {type(token)}\")\n",
    "\n",
    "gender_data = GenderDataset(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114.59587087737538, 17.034593549416673, 128, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lens =  [len(doc) for doc,_ in gender_data]\n",
    "lens = np.array(lens)\n",
    "lens.mean(), lens.std(), lens.max(), lens.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a pair of 128-dim vector of indices and a gender label: ([58430, 45267, 58809, 77229, 21467, 6736, 32732, 73874, 58077, 59426, 68442, 21467, 11245, 32399, 59426, 31956, 55413, 49670, 53510, 55413, 76617, 59804, 28171, 34420, 68442, 43731, 9120, 11245, 59426, 81851, 18060, 77727, 15045, 51681, 31956, 16452, 55413, 4830, 16893, 63852, 55413, 36453, 60389, 59426, 45972, 70433, 45267, 13442, 55413, 4830, 16893, 63852, 59426, 34420, 40269, 3801, 21553, 59426, 61123, 27520, 6464, 59426, 31956, 84366, 28681, 69027, 32085, 27756, 76076, 77229, 21467, 23903, 59804, 15000, 59804, 8692, 55413, 1895, 59426, 67367, 3801, 59426, 51487, 32085, 21752, 32085, 59426, 32085, 59426, 18060, 28993, 45267, 57143, 79666, 72935, 33527, 55413, 77562, 59804, 57143, 12282, 67423, 59426, 57143, 18060, 31871, 69027, 10767, 58528, 82382, 81851, 7752, 80672, 81851, 59426, 66565], 0) \n",
      "\n",
      "You can use GenderData.tokenize to retokenize the data: <SOS> i took an i.q. test the other day <SEP> my i.q. is 144 <SEP> and to add insult to injury , guess what my intellectual description is <SEP> \" you are highly gifted and appearing to be a genius to most people <SEP> hmmm <SEP>. i appear to be a genius <SEP> what does that mean <SEP> try it out <SEP> and just trivia : <UNK> einstein had an i.q. of , like , 168 to 169 <SEP> beat that <SEP> :-rrb- <UNK> or <UNK> <SEP> <UNK> <SEP> you n i have argued this point to death , have n't we <SEP> have you read : ayn rand 's \" atlas shrugged \" <SEP> <EOS> \n",
      "\n",
      "Or use GenderData.detokenize detokenize it back to indices: [58430, 45267, 58809, 77229, 21467, 6736, 32732, 73874, 58077, 59426, 68442, 21467, 11245, 32399, 59426, 31956, 55413, 49670, 53510, 55413, 76617, 59804, 28171, 34420, 68442, 43731, 9120, 11245, 59426, 81851, 18060, 77727, 15045, 51681, 31956, 16452, 55413, 4830, 16893, 63852, 55413, 36453, 60389, 59426, 45972, 70433, 45267, 13442, 55413, 4830, 16893, 63852, 59426, 34420, 40269, 3801, 21553, 59426, 61123, 27520, 6464, 59426, 31956, 84366, 28681, 69027, 32085, 27756, 76076, 77229, 21467, 23903, 59804, 15000, 59804, 8692, 55413, 1895, 59426, 67367, 3801, 59426, 51487, 32085, 21752, 32085, 59426, 32085, 59426, 18060, 28993, 45267, 57143, 79666, 72935, 33527, 55413, 77562, 59804, 57143, 12282, 67423, 59426, 57143, 18060, 31871, 69027, 10767, 58528, 82382, 81851, 7752, 80672, 81851, 59426, 66565] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data is a pair of 128-dim vector of indices and a gender label:\", gender_data[0], '\\n')\n",
    "tokenized_sample = gender_data.tokenize(gender_data[0][0])\n",
    "print(\"You can use GenderData.tokenize to retokenize the data:\", tokenized_sample, '\\n')\n",
    "detokenized_sample = gender_data.detokenize(tokenized_sample)\n",
    "print(\"Or use GenderData.detokenize detokenize it back to indices:\",detokenized_sample, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
