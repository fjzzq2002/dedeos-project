{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Downloaded from https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/dataset/dataset_blog.json\n",
    "with open(os.path.join(os.curdir, \"data\", \"blog.json\"), \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "docs = json_data['docs'][1:] # I don't want to see the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting documents into paragraphs of length 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19676 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 19676/19676 [02:49<00:00, 115.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 618377\n",
      "Discarded ratio (due to MIN_LENGTH): 0.163\n",
      "Number of unique words before converting to [UNK]:  27334\n",
      "Converting words with frequencies less than 10 to [UNK]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618377/618377 [00:53<00:00, 11658.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after converting [UNK]:  25659\n",
      "Known occurrences rate 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "tokenizer = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4').tokenizer\n",
    "\n",
    "# Took me 5 min to run\n",
    "\n",
    "def stop(limit = 1, times = [0]):\n",
    "    times[0] += 1\n",
    "    assert times[0] < limit, \"STOP HERE\"\n",
    "\n",
    "\n",
    "class GenderDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, docs, PARAGRAPH_LENGTH=128, MIN_LENGTH=128, UNK_THRESHOLD=10) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #---------------------------------------------------------------------------   \n",
    "        #  Cut the documents into paragraphs of PARAGRAPH_LENGTH\n",
    "        #---------------------------------------------------------------------------     \n",
    "        doc_texts = [] # List of paragraphs\n",
    "        gender_label = []\n",
    "\n",
    "        print(f\"Cutting documents into paragraphs of length {PARAGRAPH_LENGTH}...\")\n",
    "        freq = Counter() # Count the number of times each word appears\n",
    "        discarded_doc = 0\n",
    "        for doc in tqdm(docs):\n",
    "\n",
    "            gender = int(doc['gender'] == 'male')\n",
    "            # gender_label.append(gender)\n",
    "\n",
    "            for text in (doc['rawtext']):\n",
    "                tokens = self.str2token(text)\n",
    "                i = 0\n",
    "                while i + PARAGRAPH_LENGTH < len(tokens):\n",
    "                    doc_texts.append(tokens[i : i + PARAGRAPH_LENGTH])\n",
    "                    freq.update(doc_texts[-1])\n",
    "                    gender_label.append(gender)\n",
    "                    i += PARAGRAPH_LENGTH\n",
    "                last_bit = tokens[-PARAGRAPH_LENGTH:]\n",
    "                if len(last_bit) >= MIN_LENGTH:\n",
    "                    doc_texts.append(last_bit)\n",
    "                    gender_label.append(gender)\n",
    "                    freq.update(doc_texts[-1])\n",
    "                else:\n",
    "                    discarded_doc += 1\n",
    "        \n",
    "        print(f\"Number of documents: {len(doc_texts)}\")\n",
    "        print(f\"Discarded ratio (due to MIN_LENGTH): {round((discarded_doc) / (discarded_doc + len(doc_texts)), 3)}\")\n",
    "        \n",
    "\n",
    "        #---------------------------------------------------------------------------   \n",
    "        #  Convert words to [UNK], then to indices\n",
    "        #---------------------------------------------------------------------------     \n",
    "        print(\"Number of unique words before converting to [UNK]: \", len(freq))\n",
    "        before_occur = sum(freq.values())\n",
    "\n",
    "        unique_words = set()\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        print(f\"Converting words with frequencies less than {UNK_THRESHOLD} to [UNK]...\")\n",
    "        total_occur = before_occur\n",
    "        for i, doc_text in enumerate(tqdm(doc_texts)):\n",
    "            # Replace words with less than 5 occurrences with [UNK]\n",
    "            doc_text = [word if freq[word] > UNK_THRESHOLD else \"[UNK]\" for word in doc_text]\n",
    "            unique_words.update(doc_text)\n",
    "            total_occur -= doc_text.count(\"[UNK]\")\n",
    "            doc_texts[i] = doc_text\n",
    "            ids.append(self.token2idx(doc_text))\n",
    "\n",
    "        print(\"Number of unique words after converting [UNK]: \", len(unique_words))\n",
    "        print(f\"Known occurrences rate {round(total_occur/before_occur * 100, 2)}%\")\n",
    "\n",
    "        self._vocab_size = len(unique_words) # numbers of unique words == len(token2idx)\n",
    "        self._vocab = unique_words # set of unique words\n",
    "        self.raw_tokens = doc_texts # list of list of string tokens\n",
    "        self.ids = ids # list of list of ints\n",
    "        self.label = gender_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.label[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx in range(len(self)):\n",
    "            yield self[idx]\n",
    "\n",
    "    def str2idx(self, s: str):\n",
    "        return tokenizer.encode(s)[1:-1]\n",
    "    \n",
    "    def str2token(self, s: str):\n",
    "        return tokenizer.convert_ids_to_tokens(self.str2idx(s))\n",
    "    \n",
    "    def token2idx(self, tokens : list[str]):\n",
    "        return tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    def idx2str(self, idx):\n",
    "        return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(idx))\n",
    "\n",
    "\n",
    "def gender_data_collate_fn(gender_data):\n",
    "    src_len = torch.tensor([len(gender_datum[0]) for gender_datum in gender_data], dtype=torch.int32)\n",
    "    max_len = max(src_len)\n",
    "    src_ids = torch.stack([torch.cat([\n",
    "        torch.tensor(gender_datum[0], dtype=torch.int32), \n",
    "        torch.zeros(max_len - len(gender_datum[0]), dtype=torch.int32)\n",
    "        ]) for gender_datum in gender_data])\n",
    "    tgt = torch.tensor([gender_datum[1] for gender_datum in gender_data])\n",
    "    return src_ids, src_len, tgt\n",
    "\n",
    "gender_data = GenderDataset(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128.0, 0.0, 128, 128)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lens =  [len(doc) for doc,_ in gender_data]\n",
    "lens = np.array(lens)\n",
    "lens.mean(), lens.std(), lens.max(), lens.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a pair of 128-dim vector of indices and a gender label: ([1045, 2165, 2019, 1045, 1012, 1053, 1012, 3231, 1996, 2060, 2154, 1012, 1012, 1012, 1012, 2026, 1045, 1012, 1053, 1012, 2003, 14748, 1012, 1998, 2000, 5587, 15301, 2000, 4544, 1010, 3984, 2054, 2026, 7789, 6412, 2003, 1012, 1000, 2017, 2024, 3811, 12785, 1998, 6037, 2000, 2022, 1037, 11067, 2000, 2087, 2111, 1012, 17012, 2213, 1012, 1012, 1012, 1045, 3711, 2000, 2022, 1037, 11067, 1029, 2054, 2515, 2008, 2812, 1029, 3046, 2009, 2041, 1012, 1998, 2074, 13012, 9035, 1024, 2632, 5677, 15313, 2018, 2019, 1045, 1012, 1053, 1012, 1997, 1010, 2066, 1010, 16923, 2000, 18582, 1012, 3786, 2008, 1012, 1024, 1011, 25269, 2497, 1011, 7479, 18447, 13348, 17905, 22199, 1012, 4012, 2030, 7479, 18515, 22199, 1012, 4012], 0) \n",
      "\n",
      "You can use GenderData.idx2str to str-lize the data: i took an i. q. test the other day.... my i. q. is 144. and to add insult to injury, guess what my intellectual description is. \" you are highly gifted and appearing to be a genius to most people. hmmm... i appear to be a genius? what does that mean? try it out. and just trivia : alber einstein had an i. q. of, like, 168 to 169. beat that. : - rrb - wwwintelligencetest. com or wwwiqtest. com \n",
      "\n",
      "Or use GenderData.str2token convert strings to tokens: ['i', 'took', 'an', 'i', '.', 'q', '.', 'test', 'the', 'other', 'day', '.', '.', '.', '.', 'my', 'i', '.', 'q', '.', 'is', '144', '.', 'and', 'to', 'add', 'insult', 'to', 'injury', ',', 'guess', 'what', 'my', 'intellectual', 'description', 'is', '.', '\"', 'you', 'are', 'highly', 'gifted', 'and', 'appearing', 'to', 'be', 'a', 'genius', 'to', 'most', 'people', '.', 'hmm', '##m', '.', '.', '.', 'i', 'appear', 'to', 'be', 'a', 'genius', '?', 'what', 'does', 'that', 'mean', '?', 'try', 'it', 'out', '.', 'and', 'just', 'tri', '##via', ':', 'al', '##ber', 'einstein', 'had', 'an', 'i', '.', 'q', '.', 'of', ',', 'like', ',', '168', 'to', '169', '.', 'beat', 'that', '.', ':', '-', 'rr', '##b', '-', 'www', '##int', '##elli', '##gence', '##test', '.', 'com', 'or', 'www', '##iq', '##test', '.', 'com'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data is a pair of 128-dim vector of indices and a gender label:\", gender_data[0], '\\n')\n",
    "tokenized_sample = gender_data.idx2str(gender_data[0][0])\n",
    "print(\"You can use GenderData.idx2str to str-lize the data:\", tokenized_sample, '\\n')\n",
    "detokenized_sample = gender_data.str2token(tokenized_sample)\n",
    "print(\"Or use GenderData.str2token convert strings to tokens:\",detokenized_sample, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
